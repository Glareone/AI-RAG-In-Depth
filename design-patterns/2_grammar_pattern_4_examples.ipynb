{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Pattern 2. Grammar: 5 Examples (Including Deep Dive)\n\n## Educational Arc: From Practice to Understanding\n\nThis notebook demonstrates the Grammar Pattern with **5 practical examples** that progress from high-level usage to foundational understanding:\n\n1. **Insurance Forms** - Complex nested JSON extraction (Pydantic schemas)\n2. **SQL Query Generation** - Generate safe SQL with BNF Grammar Pattern (outlines + llama-cpp)\n3. **Pipe-Separated Data** - Extract structured data with strict format (Pydantic)\n4. **English Grammar Correction** - Fix grammar with output constraints (outlines + regex)\n5. **üéì Math Expression Generation** - **Deep dive into direct logits processing**\n\n### üìö Abstraction Levels\n\n| Level | Approach | Examples | What You See |\n|-------|----------|----------|--------------|\n| **High** | Pydantic + API | 1, 3 | Define schema, get structured output |\n| **Medium** | outlines/llama-cpp | 2, 4 | Define grammar/regex, library handles rest |\n| **Low** | Direct logits | **5** | **See HOW it works under the hood** |\n\n**üí° Key Insight:** Example 5 reveals the **foundational mechanism** that `outlines` and `llama-cpp` use internally. Understanding this makes you appreciate why grammar constraints provide 100% guarantees!\n\n---\n\n## Notebook ideas\n\n1) compare Structured Output and True Grammar Pattern.\n2) demonstrate how to use True Grammar Pattern using llama-cpp and outlines.\n3) üéì **Deep Dive**: Understand how it works internally with direct logits processing (Example 5)\n\n## Cases + Ideas\n* Guaranteed Format and Rule Compliance\n* Generate Proper SQL Query\n* Fix grammar in your input sentence (generating the correct one on output)\n* Understand the underlying rules (math, natural language, query language, etc)\n\n**Key Learning:** Grammar Pattern provides 100% guarantee of valid output format through token-level logits masking.\n\n## Approach Comparison. Structured output vs Grammar Pattern\n\n|  Feature | Grammar Pattern         | Structured Outputs                |\n|----------|-------------------------|-----------------------------------|\n|          | (outlines)              | (Azure OpenAI)                    |\n| Implementation           | Self-hosted model       | Azure OpenAI API                  |\n| Constraint Type          | Token-level grammar     | Schema + parsing                  |\n| Safety Guarantee         | ‚úÖ HARD (impossible)     | ‚ö†Ô∏è  SOFT (almost impossible, 99%) |\n| Grammar Support          | ‚úÖ BNF, regex, FSM       | ‚ùå Not supported                   |\n| Dangerous SQL Blocking   | ‚úÖ Physically blocked    | ‚ö†Ô∏è  Relies on prompt              |\n| Output Structure         | ‚ö†Ô∏è  Text (grammar)      | ‚úÖ Pydantic objects                |\n| Setup Complexity         | Medium (install)        | Low (just API)                    |\n| Runtime Performance      | Local (fast)            | API call (latency)                |\n| Cost                     | Hardware only           | Per-token pricing                 |\n| Model Control            | ‚úÖ Full control          | ‚ùå Server-side only                |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Install required packages\n!pip install openai pydantic python-dotenv transformers torch accelerate pandas outlines huggingface_hub\n\n# For Example 5: Direct logits processing\n!pip install transformers-cfg\n\n# Optional: For llama-cpp alternative (requires manual model download)\n# !pip install llama-cpp-python"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Installation"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T18:33:32.873921Z",
     "start_time": "2026-01-15T18:33:32.417025Z"
    }
   },
   "source": [
    "# Install required packages\n",
    "!pip install openai pydantic python-dotenv transformers torch accelerate pandas outlines\n",
    "\n",
    "# Optional: For llama-cpp alternative (requires manual model download)\n",
    "# !pip install llama-cpp-python\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (1.82.1)\r\n",
      "Requirement already satisfied: pydantic in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (2.11.5)\r\n",
      "Requirement already satisfied: python-dotenv in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (1.1.0)\r\n",
      "Requirement already satisfied: transformers in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (4.57.3)\r\n",
      "Requirement already satisfied: torch in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (2.9.1)\r\n",
      "Requirement already satisfied: accelerate in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (1.12.0)\r\n",
      "Requirement already satisfied: pandas in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (2.3.3)\r\n",
      "Requirement already satisfied: outlines in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (1.2.9)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from openai) (4.9.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from openai) (1.9.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from openai) (0.28.1)\r\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from openai) (0.10.0)\r\n",
      "Requirement already satisfied: sniffio in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from openai) (1.3.1)\r\n",
      "Requirement already satisfied: tqdm>4 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from openai) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from openai) (4.14.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from pydantic) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from pydantic) (2.33.2)\r\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from pydantic) (0.4.1)\r\n",
      "Requirement already satisfied: idna>=2.8 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\r\n",
      "Requirement already satisfied: certifi in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\r\n",
      "Requirement already satisfied: httpcore==1.* in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\r\n",
      "Requirement already satisfied: h11>=0.16 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\r\n",
      "Requirement already satisfied: filelock in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from transformers) (3.20.2)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from transformers) (0.36.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from transformers) (2.3.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from transformers) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from transformers) (0.22.2)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from transformers) (0.7.0)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.12.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from torch) (1.14.0)\r\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from torch) (3.6.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from torch) (3.1.6)\r\n",
      "Requirement already satisfied: psutil in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from accelerate) (7.0.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from pandas) (2025.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from pandas) (2025.3)\r\n",
      "Requirement already satisfied: cloudpickle in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from outlines) (3.1.2)\r\n",
      "Requirement already satisfied: diskcache in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from outlines) (5.6.3)\r\n",
      "Requirement already satisfied: jsonschema in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from outlines) (4.24.0)\r\n",
      "Requirement already satisfied: pillow in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from outlines) (12.1.0)\r\n",
      "Requirement already satisfied: outlines_core==0.2.11 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from outlines) (0.2.11)\r\n",
      "Requirement already satisfied: genson in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from outlines) (1.3.0)\r\n",
      "Requirement already satisfied: jsonpath_ng in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from outlines) (1.7.0)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\r\n",
      "Requirement already satisfied: ply in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from jsonpath_ng->outlines) (3.11)\r\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from jsonschema->outlines) (25.3.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from jsonschema->outlines) (2025.4.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from jsonschema->outlines) (0.36.2)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from jsonschema->outlines) (0.25.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from requests->transformers) (3.4.2)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\r\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup for Structured Output. This Configuration is used in several places"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T18:34:00.325500Z",
     "start_time": "2026-01-15T18:33:59.938241Z"
    }
   },
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Literal\n",
    "from enum import Enum\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key is set\n",
    "if not os.getenv(\"AZURE_OPENAI_API_KEY\") or not os.getenv(\"AZURE_OPENAI_ENDPOINT\") or not os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"):\n",
    "    raise ValueError(\"Please set AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT_NAME environment variable\")\n",
    "\n",
    "# Setup Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# gpt-4o, 4o-mini, 4.1-mini, and others could be used with slightly different results\n",
    "MODEL = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete!\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Example 1: Insurance Forms - Complex Nested JSON Extraction\n",
    "## Uses approach named Text Generation Inference (TGI) or Structured Output\n",
    "### Structured Output: This approach works in OpenAI/Bedrock models using structured output\n",
    "### Text Generation Inference (TGI): It also works in llama-cpp models through TGI\n",
    "\n",
    "## Business Problem\n",
    "\n",
    "1. Different fields, complex nesting\n",
    "Insurance companies process thousands of claim forms daily. These forms contain:\n",
    "  - Personal information, field mapping.\n",
    "  - Multiple incidents (car accidents often have multiple vehicles)\n",
    "  - Nested damage descriptions\n",
    "  - Medical records\n",
    "  - Financial details\n",
    "\n",
    "2. Make the model less verbal in places where it's not necessary\n",
    "  - Do not put extra words, just generate requested content.\n",
    "\n",
    "**Challenge:** Parsing errors cause claim delays. Extra wording\n",
    "\n",
    "**Solution:** Grammar Pattern with Pydantic Schema guarantees valid structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Example 1. Step 1. Define Complex Insurance Schema"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Enums for controlled vocabularies\n",
    "class ClaimType(str, Enum):\n",
    "    AUTO = \"auto\"\n",
    "    HEALTH = \"health\"\n",
    "    HOME = \"home\"\n",
    "    LIFE = \"life\"\n",
    "\n",
    "class IncidentSeverity(str, Enum):\n",
    "    MINOR = \"minor\"\n",
    "    MODERATE = \"moderate\"\n",
    "    SEVERE = \"severe\"\n",
    "    TOTAL_LOSS = \"total_loss\"\n",
    "\n",
    "class InjuryType(str, Enum):\n",
    "    NONE = \"none\"\n",
    "    MINOR = \"minor\"\n",
    "    SERIOUS = \"serious\"\n",
    "    CRITICAL = \"critical\"\n",
    "\n",
    "# Nested structures - No @dataclass needed with BaseModel!\n",
    "class PersonalInfo(BaseModel):\n",
    "    full_name: str = Field(description=\"Full legal name\")\n",
    "    policy_number: str = Field(description=\"Insurance policy number\")\n",
    "    phone: str = Field(description=\"Contact phone number\")\n",
    "    email: Optional[str] = Field(default=None, description=\"Email address\")\n",
    "\n",
    "class Address(BaseModel):\n",
    "    street: str\n",
    "    city: str\n",
    "    state: str\n",
    "    zip_code: str\n",
    "\n",
    "class VehicleInfo(BaseModel):\n",
    "    license_plate: str\n",
    "    vin: Optional[str] = Field(default=None, description=\"Vehicle Identification Number\")\n",
    "    make: str = Field(description=\"Vehicle manufacturer\")\n",
    "    model: str = Field(description=\"Vehicle model\")\n",
    "    year: int = Field(ge=1900, le=2030, description=\"Model year\")\n",
    "\n",
    "class DamageItem(BaseModel):\n",
    "    component: str = Field(description=\"Damaged component (e.g., 'front bumper', 'windshield')\")\n",
    "    description: str = Field(description=\"Detailed damage description\")\n",
    "    estimated_cost: float = Field(ge=0, description=\"Estimated repair cost in USD\")\n",
    "\n",
    "class Injury(BaseModel):\n",
    "    person_name: str\n",
    "    injury_type: InjuryType\n",
    "    description: str\n",
    "    medical_facility: Optional[str] = Field(default=None, description=\"Hospital or clinic name\")\n",
    "\n",
    "class Incident(BaseModel):\n",
    "    incident_location: Address\n",
    "    severity: IncidentSeverity\n",
    "    weather_conditions: Optional[str] = Field(default=None)\n",
    "    description: str = Field(description=\"Detailed description of what happened\")\n",
    "    police_report_filed: bool = Field(default=False, description=\"Whether police report was filed\")\n",
    "    police_report_number: Optional[str] = Field(default=None, description=\"Police report number if filed\")\n",
    "    incident_date: str = Field(description=\"Date of the incident in the following format (YYYY-MM-DD)\")\n",
    "\n",
    "class AutoIncident(Incident):\n",
    "    vehicles_involved: List[VehicleInfo] = Field(min_length=1)\n",
    "    damages: List[DamageItem] = Field(description=\"List of damages to each vehicle\")\n",
    "    injuries: List[Injury] = Field(default=[], description=\"List of injuries\")\n",
    "    other_driver_info: Optional[PersonalInfo] = Field(default=None)\n",
    "\n",
    "# Main claim structure\n",
    "class InsuranceClaim(BaseModel):\n",
    "    claim_type: ClaimType\n",
    "    claimant: PersonalInfo\n",
    "    incident: AutoIncident\n",
    "    claim_id: str = Field(description=\"Unique claim identifier\")\n",
    "    filing_date: str = Field(description=\"Date claim was filed (YYYY-MM-DD)\")\n",
    "    total_estimated_cost: float = Field(ge=0, description=\"Total estimated cost in USD\")\n",
    "    priority: Literal[\"low\", \"medium\", \"high\", \"urgent\"] = Field(\n",
    "        default=\"medium\",\n",
    "        description=\"Claim priority level\"\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Complex insurance schema setup is finished!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1. Step 2. Sample Insurance Claim Form (Unstructured Text)\n",
    "The idea is to extract JSON-formatted output which fits our schema to be processed further"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "insurance_form_text = \"\"\"\n",
    "CLAIM REPORT - AUTO ACCIDENT\n",
    "\n",
    "Date Filed: January 5, 2026\n",
    "Claim Reference: CLM-2026-00472\n",
    "\n",
    "CLAIMANT INFORMATION:\n",
    "Name: Sarah Johnson\n",
    "Policy #: POL-847392-AZ\n",
    "Contact: (555) 123-4567\n",
    "Email: sarah.johnson@email.com\n",
    "\n",
    "INCIDENT DETAILS:\n",
    "Date of Accident: December 28, 2025\n",
    "Location: 1234 Main Street, Phoenix, Arizona, 85001\n",
    "Weather: Rainy conditions, reduced visibility\n",
    "Police Report: Yes, Report #PX-2025-9847\n",
    "\n",
    "DESCRIPTION:\n",
    "I was driving northbound on Main Street at approximately 3:30 PM when another vehicle \n",
    "ran a red light and struck my vehicle on the passenger side. The impact caused \n",
    "significant damage to both vehicles. The other driver admitted fault at the scene.\n",
    "\n",
    "MY VEHICLE:\n",
    "2023 Toyota Camry\n",
    "License Plate: ABC-1234\n",
    "VIN: 1HGBH41JXMN109186\n",
    "\n",
    "DAMAGES TO MY VEHICLE:\n",
    "1. Front passenger door - Major dent and paint damage - Estimated $2,500\n",
    "2. Rear passenger door - Moderate dent - Estimated $1,800\n",
    "3. Passenger side mirror - Broken, needs replacement - Estimated $450\n",
    "4. Front passenger window - Shattered - Estimated $350\n",
    "\n",
    "OTHER VEHICLE:\n",
    "2021 Honda Civic\n",
    "License Plate: XYZ-9876\n",
    "Driver: Michael Chen\n",
    "Driver's Policy: POL-293847-CA\n",
    "Driver's Phone: (555) 987-6543\n",
    "\n",
    "DAMAGES TO OTHER VEHICLE:\n",
    "1. Front bumper - Completely destroyed - Estimated $1,200\n",
    "2. Hood - Crumpled - Estimated $2,000\n",
    "3. Headlight assembly - Both broken - Estimated $800\n",
    "\n",
    "INJURIES:\n",
    "1. Sarah Johnson (me) - Minor whiplash and bruising - Treated at Phoenix General Hospital\n",
    "2. Passenger Emma Johnson (my daughter, age 8) - Minor cuts from broken glass - \n",
    "   Treated at Phoenix General Hospital\n",
    "\n",
    "SEVERITY ASSESSMENT: Moderate - vehicles drivable but require significant repairs\n",
    "\n",
    "TOTAL ESTIMATED DAMAGES: $9,100\n",
    "\n",
    "Priority: HIGH (injuries involved)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Sample Insurance Form Loaded\")\n",
    "print(\"=\" * 80)\n",
    "print(insurance_form_text[:500] + \"...\")\n",
    "print(\"\\nüìÑ This unstructured text needs to be parsed into structured InsuranceClaim object\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1. Step 3. \"Option 1, Pydantic Schema\".\n",
    "### Extract with Pydantic Schema and Grammar Pattern (Guaranteed Valid output)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def extract_insurance_claim(form_text: str) -> InsuranceClaim:\n",
    "    \"\"\"\n",
    "    Extract insurance claim with Grammar Pattern guarantee.\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert insurance claim processing system. Extract all relevant information from the claim form.\n",
    "    \n",
    "    ## Insurance Form Structure:\n",
    "    \n",
    "    The insurance claim form typically contains the following sections:\n",
    "    \n",
    "    ### 1. HEADER SECTION\n",
    "    - Claim ID/Reference number (e.g., \"CLM-2026-00472\")\n",
    "    - Filing date (when the claim was submitted)\n",
    "    \n",
    "    ### 2. CLAIMANT INFORMATION SECTION\n",
    "    - Full legal name of the person filing the claim\n",
    "    - Policy number (format: POL-XXXXXX-XX)\n",
    "    - Contact phone number (format: (XXX) XXX-XXXX)\n",
    "    - Email address (optional)\n",
    "    \n",
    "    ### 3. INCIDENT DETAILS SECTION\n",
    "    - Date of accident/incident (YYYY-MM-DD format)\n",
    "    - Location: full address including street, city, state, and zip code\n",
    "    - Weather conditions at time of incident (optional)\n",
    "    - Police report information (whether filed, report number if available)\n",
    "    - Detailed narrative description of what happened\n",
    "    \n",
    "    ### 4. VEHICLES INVOLVED (for auto claims)\n",
    "    Each vehicle section includes:\n",
    "    - Year, Make, Model (e.g., \"2023 Toyota Camry\")\n",
    "    - License plate number\n",
    "    - VIN (Vehicle Identification Number) - optional\n",
    "    - For other vehicles: driver name, driver's policy number, contact info\n",
    "    \n",
    "    ### 5. DAMAGES SECTION\n",
    "    List of damaged components for each vehicle:\n",
    "    - Component name (e.g., \"front bumper\", \"passenger door\", \"windshield\")\n",
    "    - Detailed description of the damage\n",
    "    - Estimated repair cost in USD (numeric value)\n",
    "    \n",
    "    ### 6. INJURIES SECTION (if applicable)\n",
    "    For each injured person:\n",
    "    - Person's name\n",
    "    - Type of injury (none, minor, serious, critical)\n",
    "    - Description of injuries\n",
    "    - Medical facility where treated (hospital/clinic name) - optional\n",
    "    \n",
    "    ### 7. ASSESSMENT SECTION\n",
    "    - Severity classification (minor, moderate, severe, total_loss)\n",
    "    - Total estimated damages (sum of all repair costs)\n",
    "    - Priority level (low, medium, high, urgent) - often based on injuries and severity\n",
    "    \n",
    "    ## Extraction Rules:\n",
    "    \n",
    "    1. **Be thorough**: Extract ALL vehicles mentioned, even if it's the other driver's vehicle\n",
    "    2. **Accuracy**: Use exact values from the form - don't approximate or round numbers\n",
    "    3. **Dates**: Convert all dates to YYYY-MM-DD format\n",
    "    4. **Costs**: Extract numeric values only, convert to float (remove $, commas)\n",
    "    5. **Enums**: Map text to correct enum values:\n",
    "       - Claim type: auto, health, home, life\n",
    "       - Severity: minor, moderate, severe, total_loss\n",
    "       - Injury type: none, minor, serious, critical\n",
    "    6. **Missing data**: Use appropriate defaults:\n",
    "       - Optional fields can be null\n",
    "       - Use empty lists [] for injuries if none reported\n",
    "    7. **Priority**: Infer from context:\n",
    "       - urgent: life-threatening injuries or total loss\n",
    "       - high: any injuries or severe damage\n",
    "       - medium: moderate damage, no injuries\n",
    "       - low: minor damage only\n",
    "    \n",
    "    ## Common Patterns to Watch For:\n",
    "    \n",
    "    - \"MY VEHICLE\" vs \"OTHER VEHICLE\" sections - both should be included\n",
    "    - Damage estimates may be listed per item or as subtotals\n",
    "    - Police report: \"Yes\" means filed=true, extract the report number\n",
    "    - Multiple people may be injured in a single incident\n",
    "    - The claimant's vehicle should be listed first in vehicles_involved\n",
    "    \n",
    "    Extract with precision and completeness.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": form_text}\n",
    "        ],\n",
    "        response_format=InsuranceClaim,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.parsed\n",
    "\n",
    "print(\"Extracting insurance claim with enhanced system prompt...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "claim = extract_insurance_claim(insurance_form_text)\n",
    "\n",
    "print(\"\\n‚úÖ SUCCESSFULLY EXTRACTED!\\n\")\n",
    "print(f\"Claim ID: {claim.claim_id}\")\n",
    "print(f\"Type: {claim.claim_type.value}\")\n",
    "print(f\"Priority: {claim.priority}\")\n",
    "print(f\"\\nClaimant: {claim.claimant.full_name}\")\n",
    "print(f\"Policy: {claim.claimant.policy_number}\")\n",
    "print(f\"\\nIncident Date: {claim.incident.incident_date}\")\n",
    "print(f\"Location: {claim.incident.incident_location.street}, {claim.incident.incident_location.city}\")\n",
    "print(f\"Severity: {claim.incident.severity.value}\")\n",
    "print(f\"Police Report: {'Yes' if claim.incident.police_report_filed else 'No'}\")\n",
    "\n",
    "print(f\"\\nVehicles Involved: {len(claim.incident.vehicles_involved)}\")\n",
    "for i, vehicle in enumerate(claim.incident.vehicles_involved, 1):\n",
    "    print(f\"  {i}. {vehicle.year} {vehicle.make} {vehicle.model} ({vehicle.license_plate})\")\n",
    "\n",
    "print(f\"\\nDamages: {len(claim.incident.damages)}\")\n",
    "for i, damage in enumerate(claim.incident.damages, 1):\n",
    "    print(f\"  {i}. {damage.component}: ${damage.estimated_cost:,.2f}\")\n",
    "\n",
    "print(f\"\\nInjuries: {len(claim.incident.injuries)}\")\n",
    "for i, injury in enumerate(claim.incident.injuries, 1):\n",
    "    print(f\"  {i}. {injury.person_name}: {injury.injury_type.value} - {injury.description[:50]}...\")\n",
    "\n",
    "print(f\"\\nTotal Estimated Cost: ${claim.total_estimated_cost:,.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüéØ ENHANCED SYSTEM PROMPT BENEFITS:\")\n",
    "print(\"  ‚úÖ Explains insurance form structure in detail\")\n",
    "print(\"  ‚úÖ Provides clear extraction rules\")\n",
    "print(\"  ‚úÖ Guides on handling edge cases (MY VEHICLE vs OTHER VEHICLE)\")\n",
    "print(\"  ‚úÖ Specifies date and number formatting\")\n",
    "print(\"  ‚úÖ Maps common phrases to enum values\")\n",
    "print(\"  ‚úÖ Improves accuracy with context-specific instructions\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1. Step 3. \"Option 2: Json-Object\".\n",
    "### Compare: JSON Mode With Pydantic Classes + Grammar Pattern\n",
    "in order to use Json Mode you have to specify response format this way:\n",
    "\n",
    "`response_format={\"type\": \"json_object\"}`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Simulate WITHOUT grammar pattern (just JSON mode)\n",
    "def extract_without_grammar(form_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract without schema constraint - just asks for JSON.\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"\n",
    "    Extract insurance claim information and return it in JSON format.\n",
    "    Include: claim_id, claimant info, vehicles, damages, injuries, costs.\n",
    "    Return valid JSON only.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": form_text}\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"Extracting WITHOUT grammar pattern...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "json_result = extract_without_grammar(insurance_form_text)\n",
    "print(\"\\nRaw JSON output:\")\n",
    "print(json.dumps(json.loads(json_result), indent=2))\n",
    "\n",
    "# Try to parse into our schema\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n‚ö†Ô∏è PROBLEMS WITHOUT GRAMMAR PATTERN:\")\n",
    "try:\n",
    "    parsed_dict = json.loads(json_result)\n",
    "    print(\"‚úÖ Valid JSON\")\n",
    "    \n",
    "    # But check if it matches our schema\n",
    "    try:\n",
    "        claim_obj = InsuranceClaim(**parsed_dict)\n",
    "        print(\"‚úÖ Matches InsuranceClaim schema (lucky!)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Does NOT match InsuranceClaim schema!\")\n",
    "        print(f\"   Error: {str(e)[:200]}...\")\n",
    "        \n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"‚ùå Invalid JSON: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Example 1. Step 4. Export to JSON for downstream systems"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Export the claim\n",
    "claim_json = claim.model_dump_json(indent=2)\n",
    "\n",
    "print(\"Final Structured Claim (ready for downstream systems):\")\n",
    "print(\"=\" * 80)\n",
    "print(claim_json)\n",
    "\n",
    "# Save to file\n",
    "with open('insurance_claim.json', 'w') as f:\n",
    "    f.write(claim_json)\n",
    "\n",
    "print(\"\\n‚úÖ Saved to insurance_claim.json\")\n",
    "print(\"\\nüéØ This JSON is GUARANTEED to be valid and processable!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Example 2: SQL Query Generation with TRUE BNF Grammar Pattern\n",
    "\n",
    "## Business Problem\n",
    "\n",
    "1. Generate Safe queries to fetch natural data using user's instructions.\n",
    "2. Natural language to SQL system should generate syntactically correct queries:\n",
    "- No Wrong column names\n",
    "- Only valid SQL syntax\n",
    "- Not Malformed WHERE clauses\n",
    "- No dangerous operations. **DANGEROUS: DELETE, UPDATE, DROP operations from malicious/confused prompts**\n",
    "\n",
    "**Solution:** Use **TRUE BNF Grammar Pattern** with llama-cpp to guarantee safe, valid SQL.\n",
    "\n",
    "## What We'll Demonstrate\n",
    "\n",
    "Unlike Example 1 (Structured Outputs with Azure OpenAI), this example uses:\n",
    "- **Self-hosted model** (Qwen 2.5-3B in GGUF format)\n",
    "- **llama-cpp-python** library\n",
    "- **BNF Grammar** as a hard constraint (using outlines for Grammar)\n",
    "\n",
    "This is the **REAL Grammar Pattern** from the article you mentioned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Sample Database & Define BNF Grammar\n",
    "\n",
    "We'll create an in-memory SQLite database and define the BNF grammar for safe SQL."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================================\n",
    "# Part A: Create Sample Database\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"PART A: CREATING SAMPLE DATABASE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create in-memory SQLite database\n",
    "conn = sqlite3.connect(':memory:')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create tables\n",
    "cursor.execute('''\n",
    "CREATE TABLE employees (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    name TEXT NOT NULL,\n",
    "    department TEXT NOT NULL,\n",
    "    salary REAL NOT NULL,\n",
    "    hire_date TEXT NOT NULL,\n",
    "    manager_id INTEGER\n",
    ")\n",
    "''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE departments (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    name TEXT NOT NULL,\n",
    "    budget REAL NOT NULL,\n",
    "    location TEXT NOT NULL\n",
    ")\n",
    "''')\n",
    "\n",
    "# Insert sample data\n",
    "employees_data = [\n",
    "    (1, 'John Smith', 'Engineering', 95000, '2020-01-15', None),\n",
    "    (2, 'Sarah Johnson', 'Engineering', 87000, '2021-03-20', 1),\n",
    "    (3, 'Michael Chen', 'Engineering', 82000, '2022-06-10', 1),\n",
    "    (4, 'Emily Brown', 'Sales', 78000, '2020-08-01', None),\n",
    "    (5, 'David Lee', 'Sales', 71000, '2021-11-15', 4),\n",
    "    (6, 'Maria Garcia', 'Sales', 69000, '2023-02-01', 4),\n",
    "    (7, 'James Wilson', 'Marketing', 76000, '2021-05-10', None),\n",
    "    (8, 'Lisa Anderson', 'Marketing', 68000, '2022-09-20', 7),\n",
    "    (9, 'Robert Taylor', 'HR', 72000, '2020-04-15', None),\n",
    "    (10, 'Jennifer Martinez', 'HR', 65000, '2023-01-10', 9)\n",
    "]\n",
    "\n",
    "cursor.executemany('INSERT INTO employees VALUES (?, ?, ?, ?, ?, ?)', employees_data)\n",
    "\n",
    "departments_data = [\n",
    "    (1, 'Engineering', 500000, 'San Francisco'),\n",
    "    (2, 'Sales', 300000, 'New York'),\n",
    "    (3, 'Marketing', 250000, 'Los Angeles'),\n",
    "    (4, 'HR', 150000, 'Chicago')\n",
    "]\n",
    "\n",
    "cursor.executemany('INSERT INTO departments VALUES (?, ?, ?, ?)', departments_data)\n",
    "conn.commit()\n",
    "\n",
    "print(\"‚úÖ Sample database created!\")\n",
    "print(\"\\nTables:\")\n",
    "print(\"  ‚Ä¢ employees (id, name, department, salary, hire_date, manager_id)\")\n",
    "print(\"  ‚Ä¢ departments (id, name, budget, location)\")\n",
    "\n",
    "print(\"\\nSample data:\")\n",
    "print(pd.read_sql_query(\"SELECT * FROM employees LIMIT 3\", conn))\n",
    "print(\"\\n\", pd.read_sql_query(\"SELECT * FROM departments\", conn))\n",
    "\n",
    "# ============================================================================\n",
    "# Part B: Define BNF Grammar for Safe SQL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART B: DEFINING BNF GRAMMAR FOR SAFE SQL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# BNF Grammar for Safe SQL (SELECT only)\n",
    "SQL_BNF_GRAMMAR = \"\"\"\n",
    "root ::= ws select_statement ws\n",
    "\n",
    "select_statement ::= \"SELECT\" ws select_list ws from_clause (ws where_clause)? (ws group_by_clause)? (ws order_by_clause)? (ws limit_clause)? \";\"?\n",
    "\n",
    "select_list ::= \"*\" | column_list\n",
    "column_list ::= column_expr (ws \",\" ws column_expr)*\n",
    "column_expr ::= (identifier \".\")? identifier (ws \"AS\" ws identifier)?\n",
    "            | aggregate_func \"(\" ws (column_expr | \"*\") ws \")\" (ws \"AS\" ws identifier)?\n",
    "\n",
    "aggregate_func ::= \"COUNT\" | \"SUM\" | \"AVG\" | \"MIN\" | \"MAX\"\n",
    "\n",
    "from_clause ::= \"FROM\" ws table_ref (ws join_clause)*\n",
    "table_ref ::= identifier (ws \"AS\"? ws identifier)?\n",
    "\n",
    "join_clause ::= join_type ws \"JOIN\" ws table_ref ws \"ON\" ws condition\n",
    "join_type ::= \"INNER\" | \"LEFT\" | \"RIGHT\" | \"FULL\"\n",
    "\n",
    "where_clause ::= \"WHERE\" ws condition\n",
    "\n",
    "condition ::= simple_condition (ws logic_op ws simple_condition)*\n",
    "simple_condition ::= column_expr ws comparison_op ws value\n",
    "                  | column_expr ws \"BETWEEN\" ws value ws \"AND\" ws value\n",
    "                  | column_expr ws \"IN\" ws \"(\" ws value_list ws \")\"\n",
    "                  | \"(\" ws condition ws \")\"\n",
    "\n",
    "comparison_op ::= \"=\" | \"!=\" | \"<\" | \">\" | \"<=\" | \">=\"\n",
    "logic_op ::= \"AND\" | \"OR\"\n",
    "\n",
    "group_by_clause ::= \"GROUP BY\" ws column_list (ws \"HAVING\" ws condition)?\n",
    "\n",
    "order_by_clause ::= \"ORDER BY\" ws order_expr (ws \",\" ws order_expr)*\n",
    "order_expr ::= column_expr (ws (\"ASC\" | \"DESC\"))?\n",
    "\n",
    "limit_clause ::= \"LIMIT\" ws number (ws \"OFFSET\" ws number)?\n",
    "\n",
    "value ::= number | string_literal | identifier\n",
    "value_list ::= value (ws \",\" ws value)*\n",
    "string_literal ::= \"'\" [^']* \"'\"\n",
    "\n",
    "identifier ::= [a-zA-Z_][a-zA-Z0-9_]*\n",
    "number ::= [0-9]+ (\".\" [0-9]+)?\n",
    "\n",
    "ws ::= [ \\t\\n]*\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nüìù BNF Grammar defined!\")\n",
    "print(\"\\nüîí What this grammar enforces:\")\n",
    "print(\"   ‚úÖ root ::= ws select_statement ws\")\n",
    "print(\"      ‚Üí ONLY SELECT statements allowed\")\n",
    "print(\"      ‚Üí DELETE, UPDATE, INSERT, DROP are NOT in the grammar\")\n",
    "print(\"      ‚Üí Model CANNOT generate these tokens (logit = -inf)\")\n",
    "print(\"\\n   ‚úÖ Supports:\")\n",
    "print(\"      ‚Ä¢ WHERE clauses with conditions\")\n",
    "print(\"      ‚Ä¢ JOINs (INNER, LEFT, RIGHT, FULL)\")\n",
    "print(\"      ‚Ä¢ GROUP BY with HAVING\")\n",
    "print(\"      ‚Ä¢ ORDER BY with ASC/DESC\")\n",
    "print(\"      ‚Ä¢ LIMIT and OFFSET\")\n",
    "print(\"      ‚Ä¢ Aggregate functions (COUNT, SUM, AVG, MIN, MAX)\")\n",
    "print(\"\\n   ‚úÖ Blocks:\")\n",
    "print(\"      ‚Ä¢ DELETE (not in grammar)\")\n",
    "print(\"      ‚Ä¢ UPDATE (not in grammar)\")\n",
    "print(\"      ‚Ä¢ INSERT (not in grammar)\")\n",
    "print(\"      ‚Ä¢ DROP, TRUNCATE, ALTER, CREATE (not in grammar)\")\n",
    "\n",
    "print(\"\\nüí° Key Insight:\")\n",
    "print(\"   Since 'DELETE', 'UPDATE', etc. don't appear anywhere in the BNF,\")\n",
    "print(\"   llama-cpp will set their token probabilities to -inf (impossible).\")\n",
    "print(\"   This is TRUE token-level enforcement!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Database and Grammar are ready!\")\n",
    "print(\"=\" * 80)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Self-Hosted Model which supports grammar\n",
    "\n",
    "Now we'll load the model and use TRUE Grammar Pattern with BNF constraints."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from outlines import from_transformers\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOADING MODEL WITH OUTLINES + HUGGINGFACE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìö Using 'outlines' library for TRUE Grammar Pattern\")\n",
    "print(\"   ‚Ä¢ Supports BNF grammar, regex, and JSON schema constraints\")\n",
    "print(\"   ‚Ä¢ Token-level logits masking based on grammar rules\")\n",
    "print(\"   ‚Ä¢ Same approach used by HuggingFace TGI\")\n",
    "print(\"   ‚Ä¢ Works directly with HuggingFace transformers\")\n",
    "\n",
    "# Get HuggingFace token from environment (optional for non-gated models)\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "print(f\"\\nLoading model: {MODEL_NAME}...\")\n",
    "print(\"This may take a few minutes on first run (downloads model)...\\n\")\n",
    "\n",
    "# Step 1: Load HuggingFace tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=hf_token)\n",
    "\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 for efficiency\n",
    "    device_map=\"auto\",  # Automatically use GPU/MPS if available\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "print(\"‚úÖ HuggingFace model loaded\")\n",
    "\n",
    "# Step 2: Wrap with outlines for grammar support\n",
    "print(\"\\n‚è≥ Wrapping model with outlines...\")\n",
    "model = from_transformers(hf_model, tokenizer)\n",
    "\n",
    "print(\"\\n‚úÖ Model loaded successfully!\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Device: {hf_model.device}\")\n",
    "print(f\"   dtype: {hf_model.dtype}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Now we can use TRUE Grammar constraints!\")\n",
    "print(\"=\" * 80)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Test 1. Generate SQL with TRUE BNF Grammar Constraint\n",
    "Now we'll use the BNF grammar to generate SQL queries with HARD constraints."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from outlines import Generator, regex\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CREATING SQL GRAMMAR WITH OUTLINES (REGEX)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüîß Grammar Type: Regex (Regular Expression)\")\n",
    "print(\"   ‚Ä¢ Constrains output to match specific patterns\")\n",
    "print(\"   ‚Ä¢ Token-level logits masking (tokens that don't match = -inf)\")\n",
    "print(\"   ‚Ä¢ TRUE Grammar Pattern (not just prompt engineering)\")\n",
    "\n",
    "# Define SQL regex pattern that only allows SELECT statements\n",
    "sql_regex = r\"SELECT[\\s\\S]+FROM[\\s\\S]+\"\n",
    "\n",
    "print(\"\\nüìù SQL Regex Pattern:\")\n",
    "print(f\"   {sql_regex}\")\n",
    "\n",
    "print(\"\\nüîí This regex enforces:\")\n",
    "print(\"   ‚úÖ Must start with SELECT\")\n",
    "print(\"   ‚úÖ Must have FROM clause\")\n",
    "print(\"   ‚ùå IMPOSSIBLE: DELETE, UPDATE, INSERT, DROP, ALTER\")\n",
    "print(\"      (These keywords are not in the regex, so tokens are blocked)\")\n",
    "\n",
    "# Create generator with regex grammar\n",
    "print(\"\\n‚è≥ Compiling regex grammar...\")\n",
    "sql_generator = Generator(model, output_type=regex(sql_regex))\n",
    "print(\"‚úÖ Grammar compiled!\")\n",
    "\n",
    "print(\"\\nüí° How it works:\")\n",
    "print(\"   1. outlines compiles regex to finite state machine (FSM)\")\n",
    "print(\"   2. At each token generation:\")\n",
    "print(\"      ‚Üí FSM determines which tokens are valid\")\n",
    "print(\"      ‚Üí Invalid tokens get logit = -inf (impossible to generate)\")\n",
    "print(\"      ‚Üí Model MUST choose valid token\")\n",
    "print(\"   3. Output GUARANTEED to match regex\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TESTING: Generate SQL Queries with Grammar Constraint\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test questions - SAFE requests\n",
    "test_questions = [\n",
    "    \"Show me all employees in Engineering department\",\n",
    "    \"What is the average salary by department?\",\n",
    "    \"List employees who earn more than $75,000\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"Generate a SQL SELECT query.\n",
    "\n",
    "Database Schema:\n",
    "- employees: id, name, department, salary, hire_date, manager_id\n",
    "- departments: id, name, budget, location\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "SQL Query:\\n\"\"\"\n",
    "    \n",
    "    print(\"\\n‚è≥ Generating with regex grammar constraint...\")\n",
    "    \n",
    "    try:\n",
    "        # Generate with grammar constraint\n",
    "        sql_query = sql_generator(prompt)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Generated SQL:\")\n",
    "        print(f\"   {sql_query}\")\n",
    "        \n",
    "        # Try to execute it\n",
    "        try:\n",
    "            # Clean the query\n",
    "            clean_sql = sql_query.split(';')[0].strip()\n",
    "            if not clean_sql.upper().startswith('SELECT'):\n",
    "                clean_sql = 'SELECT' + clean_sql\n",
    "                \n",
    "            df = pd.read_sql_query(clean_sql, conn)\n",
    "            print(f\"\\nüìä Execution: SUCCESS\")\n",
    "            print(f\"   Rows returned: {len(df)}\")\n",
    "            if len(df) > 0:\n",
    "                print(f\"\\n   Results preview:\")\n",
    "                print(df.head().to_string(index=False))\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è Execution error: {str(e)[:150]}\")\n",
    "            print(\"   Note: Grammar ensures SELECT-only, but not perfect SQL syntax\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Generation failed: {str(e)[:200]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Test 2. SQL Query generation Test. BNF Grammar Block Dangerous Operations?\n",
    "\n",
    "Let's try to make the model generate dangerous SQL operations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TESTING GRAMMAR SAFETY: Attempting Dangerous Operations\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Dangerous requests that should be blocked\n",
    "dangerous_questions = [\n",
    "    \"Delete all employees from the Engineering department\",\n",
    "    \"Update all salaries to $100,000\",\n",
    "    \"Drop the employees table\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(dangerous_questions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"‚ö†Ô∏è Dangerous Request {i}: {question}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    prompt = f\"\"\"Generate SQL for this request.\n",
    "\n",
    "Database Schema:\n",
    "- employees: id, name, department, salary, hire_date, manager_id\n",
    "- departments: id, name, budget, location\n",
    "\n",
    "Request: {question}\n",
    "\n",
    "SQL Query:\\n\"\"\"\n",
    "    \n",
    "    print(\"\\n‚è≥ Attempting to generate with grammar constraint...\")\n",
    "    \n",
    "    try:\n",
    "        # Generate with grammar constraint\n",
    "        # The regex ONLY allows SELECT, so dangerous operations are impossible\n",
    "        generated_sql = sql_generator(prompt)\n",
    "        \n",
    "        print(f\"\\nüìù What the model generated:\")\n",
    "        print(f\"   {generated_sql}\")\n",
    "        \n",
    "        # Check if it's a SELECT\n",
    "        if generated_sql.strip().upper().startswith(\"SELECT\"):\n",
    "            print(f\"\\n‚úÖ SAFE: Grammar forced a SELECT query instead!\")\n",
    "            print(f\"   The model CANNOT generate DELETE/UPDATE/DROP with this regex.\")\n",
    "            print(f\"   Even when explicitly asked, grammar physically blocks it.\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è Unexpected: Not a SELECT query\")\n",
    "            print(f\"   This shouldn't happen with the regex constraint.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Generation failed: {str(e)[:150]}\")\n",
    "        print(\"   This might happen if the model can't satisfy the grammar\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üõ°Ô∏è GRAMMAR PATTERN SAFETY DEMONSTRATED:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n‚úÖ Dangerous SQL operations were BLOCKED by grammar constraint\")\n",
    "print(\"   ‚Ä¢ DELETE, UPDATE, INSERT, DROP are NOT in the regex pattern\")\n",
    "print(\"   ‚Ä¢ Tokens for these keywords get logit = -inf (impossible)\")\n",
    "print(\"   ‚Ä¢ Model physically CANNOT generate them\")\n",
    "print(\"   ‚Ä¢ This is TRUE Grammar Pattern - hard constraint at token level\")\n",
    "\n",
    "print(\"\\nüéØ Key Difference from Prompt Engineering:\")\n",
    "print(\"   ‚ùå Prompt: 'Only generate SELECT' ‚Üí Model might ignore\")\n",
    "print(\"   ‚úÖ Grammar: regex only allows SELECT ‚Üí Physically impossible to violate\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Alternative: Using llama-cpp-python with BNF Grammar\n",
    "\n",
    "The `llama-cpp-python` library provides an alternative way to use TRUE BNF Grammar Pattern.\n",
    "\n",
    "**Key Differences:**\n",
    "- `outlines`: Works with HuggingFace models (FP16/BF16), auto-downloads\n",
    "- `llama-cpp`: Works with GGUF models (quantized), requires manual download\n",
    "\n",
    "**Both provide TRUE Grammar Pattern** with token-level logits masking.\n",
    "\n",
    "### When to use llama-cpp:\n",
    "- Need smaller model size (quantized GGUF)\n",
    "- Want faster inference on CPU\n",
    "- Prefer BNF grammar over regex\n",
    "\n",
    "### Setup:\n",
    "```bash\n",
    "# Install llama-cpp-python\n",
    "pip install llama-cpp-python\n",
    "\n",
    "# Download GGUF model (example)\n",
    "# Visit: https://huggingface.co/Qwen/Qwen2.5-3B-Instruct-GGUF\n",
    "# Download: qwen2.5-3b-instruct-q5_k_m.gguf (~2.5GB)\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install llama-cpp-python",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# OPTIONAL: llama-cpp-python implementation\n# Uncomment to use this alternative approach\n\nfrom llama_cpp import Llama, LlamaGrammar\nfrom huggingface_hub import hf_hub_download\nimport os\n\nprint(\"=\" * 80)\nprint(\"LOADING MODEL WITH LLAMA-CPP + BNF GRAMMAR\")\nprint(\"=\" * 80)\n\n# Create models directory if it doesn't exist\nos.makedirs(\"models\", exist_ok=True)\n\n# Model details\nREPO_ID = \"Qwen/Qwen2.5-3B-Instruct-GGUF\"\nFILENAME = \"qwen2.5-3b-instruct-q5_k_m.gguf\"\nMODEL_PATH = f\"models/{FILENAME}\"\n\n# Download if not exists\nif not os.path.exists(MODEL_PATH):\n    print(f\"\\nüì• Downloading {FILENAME} (~2.5GB)...\")\n    print(\"   This may take 5-10 minutes depending on your connection...\")\n    MODEL_PATH = hf_hub_download(\n        repo_id=REPO_ID,\n        filename=FILENAME,\n        local_dir=\"models\",\n        local_dir_use_symlinks=False\n    )\n    print(\"‚úÖ Download complete!\")\nelse:\n    print(f\"\\n‚úÖ Model already exists at {MODEL_PATH}\")\n\nprint(f\"\\nüì¶ Loading model from: {MODEL_PATH}\")\nprint(\"   This may take 30-60 seconds...\\n\")\n\n# Load model with llama-cpp\nllm = Llama(\n    model_path=MODEL_PATH,\n    n_ctx=2048,           # Context window\n    n_threads=8,          # Use multiple threads\n    n_gpu_layers=0,       # 0 = CPU only, increase for Metal/MPS\n    verbose=False\n)\n    \nprint(\"‚úÖ Model loaded successfully!\")\n    \n# Create BNF grammar\nprint(\"\\n‚è≥ Compiling BNF grammar...\")\nsql_grammar = LlamaGrammar.from_string(SQL_BNF_GRAMMAR)\nprint(\"‚úÖ Grammar compiled!\")\n    \n# Test generation\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TESTING: Generate SQL with llama-cpp BNF Grammar\")\nprint(\"=\" * 80)\n    \ntest_question = \"What is the average salary by department?\"\n    \nprompt = f\"\"\"Generate a SQL SELECT query.\nDatabase Schema:\n- employees: id, name, department, salary, hire_date, manager_id\n- departments: id, name, budget, location\n\nQuestion: {test_question}\n\nSQL Query:\\n\"\"\"\n    \nprint(f\"\\nQuestion: {test_question}\")\nprint(\"\\n‚è≥ Generating with BNF grammar constraint...\")\n    \n# Generate with grammar constraint\nresponse = llm(\n    prompt,\n    max_tokens=150,\n    temperature=0.3,\n   grammar=sql_grammar,  # <-- BNF Grammar constraint!\n    stop=[\";\", \"\\n\\n\"]\n)\n    \nsql_query = response['choices'][0]['text'].strip()\n    \nprint(f\"\\n‚úÖ Generated SQL:\")\nprint(f\"   {sql_query}\")\n    \n# Try to execute\ntry:\n    df = pd.read_sql_query(sql_query, conn)\n    print(f\"\\nüìä Execution: SUCCESS\")\n    print(f\"   Rows returned: {len(df)}\")\n    if len(df) > 0:\n        print(f\"\\n   Results:\")\n        print(df.to_string(index=False))\nexcept Exception as e:\n    print(f\"\\n‚ö†Ô∏è Execution error: {str(e)[:150]}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ Both outlines and llama-cpp provide TRUE Grammar Pattern!\")\nprint(\"   See comparison table below for detailed differences.\")\nprint(\"=\" * 80)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Implementation Comparison: outlines vs llama-cpp\n\nBoth libraries provide TRUE Grammar Pattern with token-level logits masking.\n\n| Feature | outlines | llama-cpp |\n|---------|----------|-----------|\n| Model Format | HuggingFace models | GGUF models |\n| Precision | FP16/BF16 (full) | Quantized (Q4, Q5, Q8) |\n| Model Download | ‚úÖ Auto-downloads | ‚ö†Ô∏è Manual download required |\n| Grammar Support | Regex, JSON schema, FSM | ‚úÖ Full BNF grammar |\n| Model Size | ~6GB (full precision) | ~2.5GB (Q5 quantization) |\n| Token-level Masking | ‚úÖ Yes | ‚úÖ Yes |\n| Format Guarantee | ‚úÖ 100% guaranteed | ‚úÖ 100% guaranteed |\n| Grammar Violation | ‚ùå Physically impossible | ‚ùå Physically impossible |\n| Library Ecosystem | transformers + outlines | llama-cpp-python |\n| Hardware Optimization | GPU/MPS preferred | CPU optimized |\n| Setup Complexity | Medium (pip install) | Medium (download + pip) |\n| Best For | Latest models, GPU | Smaller footprint, CPU |\n| Memory Usage | Higher (~6-8GB) | Lower (~3-4GB) |\n| Integration | Python native | C++ backend |\n\n**Bottom Line:** Both provide TRUE Grammar Pattern. Choose based on your infrastructure:\n- **outlines**: When you want latest HuggingFace models with auto-download\n- **llama-cpp**: When you need smaller model size and CPU efficiency",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Example 3. Pipe-separated extraction\n",
    "\n",
    "Extract Named Entities from the text in the following format:\n",
    "`SKU | Product Name | Price | Category`\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Sample product descriptions\n",
    "product_descriptions = [\n",
    "    \"\"\"\n",
    "    Apple iPhone 17 Pro - Latest flagship smartphone with Pro chip,\n",
    "    48MP camera system, and titanium design. SKU: IPHONE-17-PRO-256.\n",
    "    Price: $999. Category: Electronics/Smartphones\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Samsung 65\" QLED 4K Smart TV - Quantum HDR, Object Tracking Sound+, \n",
    "    and Gaming Hub. Model: QN65Q80C. Retail price: $1,299.99\n",
    "    Category: Electronics/TVs\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Nike Air Max 270 - Men's running shoes with visible Max Air cushioning.\n",
    "    Style code: DM9652-001. Price $160. Category: Footwear/Athletic\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Organic Green Tea - Premium loose leaf tea from Japan. \n",
    "    No SKU assigned yet. Price: $24.99 per package. Category: Groceries/Beverages\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "print(\"Sample Product Descriptions:\")\n",
    "print(\"=\" * 80)\n",
    "for i, desc in enumerate(product_descriptions, 1):\n",
    "    print(f\"\\n{i}. {desc.strip()[:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüéØ Goal: Extract as pipe-separated format:\")\n",
    "print(\"   SKU | Product Name | Price | Category\")\n",
    "print(\"\\nConstraints:\")\n",
    "print(\"  ‚Ä¢ Exactly 3 pipes (4 fields)\")\n",
    "print(\"  ‚Ä¢ Use NULL for missing SKU\")\n",
    "print(\"  ‚Ä¢ Only alphanumeric and basic punctuation\")\n",
    "print(\"  ‚Ä¢ Price must be numeric\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Pipe-Separated Schema\n",
    "\n",
    "For pipe-separated format, we'll use Pydantic schema approach (simpler than BNF for this case)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ProductRecord(BaseModel):\n",
    "    \"\"\"Single product in structured format (we'll convert to pipe-separated)\"\"\"\n",
    "    sku: str = Field(description=\"Product SKU, use 'NULL' if not available\")\n",
    "    product_name: str = Field(description=\"Product name, alphanumeric only\")\n",
    "    price: float = Field(ge=0, description=\"Product price in USD\")\n",
    "    category: str = Field(description=\"Product category\")\n",
    "    \n",
    "    def to_pipe_format(self) -> str:\n",
    "        \"\"\"Convert to pipe-separated format\"\"\"\n",
    "        return f\"{self.sku}|{self.product_name}|{self.price:.2f}|{self.category}\"\n",
    "\n",
    "def extract_product_record(description: str) -> ProductRecord:\n",
    "    \"\"\"\n",
    "    Extract product information with Grammar Pattern.\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"\n",
    "    Extract product information from the description.\n",
    "    Use 'NULL' for SKU if not provided.\n",
    "    Clean product name to alphanumeric characters only.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": description}\n",
    "        ],\n",
    "        response_format=ProductRecord,\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.parsed\n",
    "\n",
    "print(\"‚úÖ Pipe-separated extraction ready!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Products in Pipe-Separated Format"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Extracting Products in Pipe-Separated Format\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "pipe_records = []\n",
    "\n",
    "for i, description in enumerate(product_descriptions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"\\nüì¶ Product {i}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(description.strip()[:150] + \"...\")\n",
    "    \n",
    "    # Extract with Grammar Pattern\n",
    "    product = extract_product_record(description)\n",
    "    \n",
    "    # Convert to pipe format\n",
    "    pipe_format = product.to_pipe_format()\n",
    "    pipe_records.append(pipe_format)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Extracted (structured):\")\n",
    "    print(f\"   SKU: {product.sku}\")\n",
    "    print(f\"   Name: {product.product_name}\")\n",
    "    print(f\"   Price: ${product.price:.2f}\")\n",
    "    print(f\"   Category: {product.category}\")\n",
    "    \n",
    "    print(f\"\\nüìÑ Pipe-separated format:\")\n",
    "    print(f\"   {pipe_format}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüìã FINAL OUTPUT (ready for legacy system import):\")\n",
    "print(\"=\" * 80)\n",
    "for record in pipe_records:\n",
    "    print(record)\n",
    "\n",
    "print(\"\\nüéØ GUARANTEES:\")\n",
    "print(\"  ‚úÖ Exactly 3 pipes in each line\")\n",
    "print(\"  ‚úÖ Price always numeric (float)\")\n",
    "print(\"  ‚úÖ NULL used for missing SKU\")\n",
    "print(\"  ‚úÖ No parsing errors\")\n",
    "print(\"  ‚úÖ Ready for direct import to legacy system\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to File"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save to PSV (Pipe-Separated Values) file\n",
    "with open('products.psv', 'w') as f:\n",
    "    f.write(\"SKU|Product Name|Price|Category\\n\")  # Header\n",
    "    for record in pipe_records:\n",
    "        f.write(record + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ Saved to products.psv\")\n",
    "print(\"\\nFile contents:\")\n",
    "with open('products.psv', 'r') as f:\n",
    "    print(f.read())\n",
    "\n",
    "print(\"\\nüéØ This file is GUARANTEED to be:\")\n",
    "print(\"  ‚úÖ Properly formatted\")\n",
    "print(\"  ‚úÖ Importable by legacy systems\")\n",
    "print(\"  ‚úÖ No manual validation needed\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Example 4: English Grammar Correction\n",
    "\n",
    "## Business Problem\n",
    "\n",
    "Grammar correction tools need to:\n",
    "- Fix grammatical errors in user input\n",
    "- Preserve the original meaning\n",
    "- Output ONLY the corrected sentence (not explanations)\n",
    "- Handle various types of errors (subject-verb agreement, tense, articles, etc.)\n",
    "\n",
    "**Challenge:** LLMs naturally want to explain their corrections or add commentary.\n",
    "\n",
    "**Solution:** Grammar Pattern constrains output to be valid English sentence only.\n",
    "\n",
    "## What We'll Demonstrate\n",
    "\n",
    "We'll use **regex grammar** with `outlines` to ensure:\n",
    "1. Output is a single sentence\n",
    "2. Starts with capital letter\n",
    "3. Ends with proper punctuation\n",
    "4. Contains only valid characters (no explanations, no bullet points)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define English Sentence Grammar\n",
    "\n",
    "We'll use regex to constrain the output to valid English sentences:\n",
    "\n",
    "```regex\n",
    "^[A-Z][A-Za-z0-9\\s,.'\"\\-!?]+[.!?]$\n",
    "```\n",
    "\n",
    "**What this regex enforces:**\n",
    "- `^[A-Z]` - Must start with capital letter\n",
    "- `[A-Za-z0-9\\s,.'\"\\-!?]+` - Contains letters, numbers, spaces, and basic punctuation\n",
    "- `[.!?]$` - Must end with sentence-ending punctuation\n",
    "\n",
    "**What it prevents:**\n",
    "- Multi-sentence responses\n",
    "- Explanations (e.g., \"The error was...\")\n",
    "- Bullet points or formatting\n",
    "- Missing capitalization or punctuation\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from outlines import Generator, regex\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CREATING ENGLISH SENTENCE GRAMMAR\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Regex for valid English sentence\n",
    "# - Starts with capital letter\n",
    "# - Contains alphanumeric, spaces, and basic punctuation\n",
    "# - Ends with . ! or ?\n",
    "english_sentence_regex = r\"[A-Z][A-Za-z0-9\\s,.'\\\"\\-!?]*[.!?]\"\n",
    "\n",
    "print(\"\\nüìù English Sentence Regex:\")\n",
    "print(f\"   {english_sentence_regex}\")\n",
    "\n",
    "print(\"\\nüîí This regex enforces:\")\n",
    "print(\"   ‚úÖ Starts with capital letter\")\n",
    "print(\"   ‚úÖ Valid characters only (letters, numbers, basic punctuation)\")\n",
    "print(\"   ‚úÖ Ends with proper punctuation (. ! ?)\")\n",
    "print(\"   ‚ùå BLOCKS: Multi-sentence responses, explanations, formatting\")\n",
    "\n",
    "print(\"\\n‚è≥ Creating grammar-constrained generator...\")\n",
    "grammar_corrector = Generator(model, output_type=regex(english_sentence_regex))\n",
    "print(\"‚úÖ Grammar generator created!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test: Grammar Correction with Constrained Output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Sample sentences with grammatical errors\n",
    "test_sentences = [\n",
    "    \"She don't like pizza.\",  # Subject-verb agreement\n",
    "    \"He go to school yesterday.\",  # Tense error\n",
    "    \"I have a apple and orange.\",  # Article error\n",
    "    \"They was happy about the news.\",  # Subject-verb agreement\n",
    "    \"Me and him went to store.\",  # Pronoun case error\n",
    "    \"The cat it is sleeping on the chair.\",  # Redundant pronoun\n",
    "    \"She have three brother and two sister.\",  # Multiple errors\n",
    "]"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CORRECTING GRAMMAR WITH CONSTRAINED OUTPUT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, incorrect_sentence in enumerate(test_sentences, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Example {i}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"‚ùå Original: {incorrect_sentence}\")\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"Fix the grammar in this sentence. Output ONLY the corrected sentence, nothing else.\n",
    "\n",
    "Incorrect: {incorrect_sentence}\n",
    "\n",
    "Corrected: \"\"\"\n",
    "    \n",
    "    print(\"\\n‚è≥ Generating with grammar constraint...\")\n",
    "    \n",
    "    try:\n",
    "        # Generate with grammar constraint\n",
    "        corrected = grammar_corrector(prompt)\n",
    "        \n",
    "        print(f\"‚úÖ Corrected: {corrected}\")\n",
    "        \n",
    "        # Verify it matches our regex\n",
    "        import re\n",
    "        if re.match(english_sentence_regex, corrected):\n",
    "            print(\"   ‚úÖ Matches grammar constraints\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  Does not match regex (shouldn't happen!)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Generation failed: {str(e)[:150]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéØ GRAMMAR PATTERN BENEFITS:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n‚úÖ Output is ALWAYS a valid sentence:\")\n",
    "print(\"   ‚Ä¢ Starts with capital letter\")\n",
    "print(\"   ‚Ä¢ Ends with proper punctuation\")\n",
    "print(\"   ‚Ä¢ No extra explanations or commentary\")\n",
    "print(\"   ‚Ä¢ Single sentence only (no paragraphs)\")\n",
    "print(\"\\n‚úÖ Physical constraint at token level:\")\n",
    "print(\"   ‚Ä¢ Model CANNOT generate invalid characters\")\n",
    "print(\"   ‚Ä¢ Model CANNOT skip capitalization or punctuation\")\n",
    "print(\"   ‚Ä¢ 100% guarantee of format compliance\")\n"
   ],
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_sentence = \"She don't like pizza.\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARISON: Grammar Pattern vs Regular Prompting\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTest sentence: {test_sentence}\")\n",
    "\n",
    "# Without Grammar Pattern (using API)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚ùå WITHOUT GRAMMAR PATTERN (regular prompting):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "response_no_grammar = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a grammar checker. Fix errors in user sentences.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Fix the grammar: {test_sentence}\"}\n",
    "    ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(response_no_grammar.choices[0].message.content)\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Problems:\")\n",
    "print(\"   ‚Ä¢ May include explanation ('The error is...')\")\n",
    "print(\"   ‚Ä¢ May format with quotes or markdown\")\n",
    "print(\"   ‚Ä¢ May include multiple variations\")\n",
    "print(\"   ‚Ä¢ Inconsistent output format\")\n",
    "print(\"   ‚Ä¢ Hard to parse programmatically\")\n",
    "\n",
    "# With Grammar Pattern\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ WITH GRAMMAR PATTERN (regex constraint):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "prompt = f\"\"\"Fix the grammar in this sentence. Output ONLY the corrected sentence.\n",
    "\n",
    "Incorrect: {test_sentence}\n",
    "\n",
    "Corrected: \"\"\"\n",
    "\n",
    "corrected = grammar_corrector(prompt)\n",
    "print(corrected)\n",
    "\n",
    "print(\"\\n‚úÖ Benefits:\")\n",
    "print(\"   ‚Ä¢ Always just the corrected sentence\")\n",
    "print(\"   ‚Ä¢ No explanations or formatting\")\n",
    "print(\"   ‚Ä¢ Consistent output structure\")\n",
    "print(\"   ‚Ä¢ Easy to parse and use\")\n",
    "print(\"   ‚Ä¢ Physical guarantee (not just prompt)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üí° KEY INSIGHT:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nRegular prompting (soft constraint):\")\n",
    "print(\"   ‚ö†Ô∏è  'Please only output the sentence' ‚Üí Model might ignore\")\n",
    "print(\"   ‚ö†Ô∏è  Relies on model's instruction following\")\n",
    "print(\"   ‚ö†Ô∏è  Can be bypassed or misunderstood\")\n",
    "print(\"\\nGrammar Pattern (hard constraint):\")\n",
    "print(\"   ‚úÖ Regex physically blocks invalid tokens\")\n",
    "print(\"   ‚úÖ Model CANNOT generate explanations\")\n",
    "print(\"   ‚úÖ 100% guaranteed format compliance\")\n",
    "print(\"\\nüéØ Use Grammar Pattern when output format MUST be exact!\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üéì Example 5: Math Expression Generation - Direct Logits Processing\n",
    "\n",
    "## Deep Dive: Understanding How Grammar Constraints Work Internally\n",
    "\n",
    "In **Examples 2 and 4**, we used the `outlines` library which provides a high-level abstraction. In the **llama-cpp alternative**, we showed another library approach.\n",
    "\n",
    "**But how do they actually work under the hood?**\n",
    "\n",
    "This example demonstrates the **foundational mechanism** that both `outlines` and `llama-cpp` use internally: **Direct Logits Processing**.\n",
    "\n",
    "## Business Problem\n",
    "\n",
    "Math tutoring applications need to:\n",
    "- Extract mathematical expressions from word problems\n",
    "- Ensure output is ONLY valid mathematical syntax (no explanations)\n",
    "- Block natural language responses\n",
    "- Generate symbolic expressions that can be evaluated programmatically\n",
    "\n",
    "**Challenge**: LLMs naturally want to explain their reasoning instead of just providing expressions.\n",
    "\n",
    "**Solution**: Use grammar constraints to physically block all tokens except valid math expressions.\n",
    "\n",
    "## What We'll Demonstrate\n",
    "\n",
    "Unlike previous examples, we'll:\n",
    "- Use **direct logits manipulation** with `GrammarConstrainedLogitsProcessor`\n",
    "- See the **low-level mechanism** that libraries abstract away\n",
    "- Understand **how grammar constraints work at the token level**\n",
    "\n",
    "## Comparison Table\n",
    "\n",
    "| Approach | Library | Abstraction Level | Examples |\n",
    "|----------|---------|-------------------|----------|\n",
    "| Pydantic Schemas | OpenAI API | High (API-based) | 1, 3 |\n",
    "| Regex/BNF Grammar | outlines | Medium (library) | 2, 4 |\n",
    "| BNF Grammar | llama-cpp | Medium (library) | 2 (alternative) |\n",
    "| **Direct Logits** | **transformers-cfg** | **Low (foundational)** | **5 (this!)** |\n",
    "\n",
    "**Key Insight**: This is what `outlines` does when you write `Generator(model, output_type=regex(...))` - it creates a logits processor similar to what we'll implement here!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T18:33:14.850320Z",
     "start_time": "2026-01-15T18:33:13.331978Z"
    }
   },
   "source": [
    "!pip install transformers-cfg"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers-cfg\r\n",
      "  Downloading transformers_cfg-0.2.7-py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: torch>=2.0.0 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from transformers-cfg) (2.9.1)\r\n",
      "Requirement already satisfied: numpy>=1.24.2 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from transformers-cfg) (2.3.0)\r\n",
      "Requirement already satisfied: transformers>=4.37.2 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from transformers-cfg) (4.57.3)\r\n",
      "Requirement already satisfied: tokenizers>=0.19.0 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from transformers-cfg) (0.22.2)\r\n",
      "Collecting termcolor>=2.4.0 (from transformers-cfg)\r\n",
      "  Downloading termcolor-3.3.0-py3-none-any.whl.metadata (6.5 kB)\r\n",
      "Requirement already satisfied: sentencepiece>=0.1.99 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from transformers-cfg) (0.2.1)\r\n",
      "Collecting protobuf>=4.25.2 (from transformers-cfg)\r\n",
      "  Downloading protobuf-6.33.4-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\r\n",
      "Requirement already satisfied: setuptools>=69.0.3 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from transformers-cfg) (78.1.1)\r\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from tokenizers>=0.19.0->transformers-cfg) (0.36.0)\r\n",
      "Requirement already satisfied: filelock in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.19.0->transformers-cfg) (3.20.2)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.19.0->transformers-cfg) (2025.12.0)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.19.0->transformers-cfg) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.19.0->transformers-cfg) (6.0.2)\r\n",
      "Requirement already satisfied: requests in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.19.0->transformers-cfg) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.19.0->transformers-cfg) (4.67.1)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.19.0->transformers-cfg) (4.14.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers>=0.19.0->transformers-cfg) (1.2.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from torch>=2.0.0->transformers-cfg) (1.14.0)\r\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from torch>=2.0.0->transformers-cfg) (3.6.1)\r\n",
      "Requirement already satisfied: jinja2 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from torch>=2.0.0->transformers-cfg) (3.1.6)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=2.0.0->transformers-cfg) (1.3.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from transformers>=4.37.2->transformers-cfg) (2024.11.6)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from transformers>=4.37.2->transformers-cfg) (0.7.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->transformers-cfg) (3.0.2)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers>=0.19.0->transformers-cfg) (3.4.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers>=0.19.0->transformers-cfg) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers>=0.19.0->transformers-cfg) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers>=0.19.0->transformers-cfg) (2025.6.15)\r\n",
      "Downloading transformers_cfg-0.2.7-py3-none-any.whl (67 kB)\r\n",
      "Downloading protobuf-6.33.4-cp39-abi3-macosx_10_9_universal2.whl (427 kB)\r\n",
      "Downloading termcolor-3.3.0-py3-none-any.whl (7.7 kB)\r\n",
      "Installing collected packages: termcolor, protobuf, transformers-cfg\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3/3\u001b[0m [transformers-cfg]\r\n",
      "\u001b[1A\u001b[2KSuccessfully installed protobuf-6.33.4 termcolor-3.3.0 transformers-cfg-0.2.7\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define BNF Grammar for Math Expressions\n",
    "\n",
    "We'll define a grammar that accepts simple arithmetic expressions:\n",
    "\n",
    "```bnf\n",
    "root ::= (expr \"=\" ws term \"\\n\")+\n",
    "expr ::= term ([-+*/] term)*\n",
    "term ::= ident | num | \"(\" ws expr \")\" ws\n",
    "ident ::= [a-z] [a-z0-9_]* ws\n",
    "num ::= [0-9]+ ws\n",
    "ws ::= [ \\t\\n]*\n",
    "```\n",
    "\n",
    "**What this grammar enforces:**\n",
    "- `root`: One or more expressions with equals signs\n",
    "- `expr`: Terms connected by +, -, *, /\n",
    "- `term`: Can be an identifier (bill_apples), a number (3), or a parenthesized expression\n",
    "- `ident`: Variable names (lowercase letters, numbers, underscores)\n",
    "- `num`: Integer numbers\n",
    "- `ws`: Whitespace\n",
    "\n",
    "**What it blocks:**\n",
    "- Natural language explanations\n",
    "- Multiple sentences\n",
    "- Anything that's not a mathematical expression"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T18:36:28.961418Z",
     "start_time": "2026-01-15T18:36:21.199408Z"
    }
   },
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers_cfg.grammar_utils import IncrementalGrammarConstraint\n",
    "from transformers_cfg.generation.logits_process import GrammarConstrainedLogitsProcessor\n",
    "import torch\n",
    "import os\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STEP 1: DEFINE BNF GRAMMAR FOR MATH EXPRESSIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define grammar for simple arithmetic expressions\n",
    "grammar_str = \"\"\"\n",
    "root ::= (expr \"=\" ws term \"\\\\n\")+\n",
    "expr ::= term ([-+*/] term)*\n",
    "term ::= ident | num | \"(\" ws expr \")\" ws\n",
    "ident ::= [a-z] [a-z0-9_]* ws\n",
    "num ::= [0-9]+ ws\n",
    "ws ::= [ \\\\t\\\\n]*\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nüìù Math Expression Grammar defined!\")\n",
    "print(\"\\nüîí This grammar enforces:\")\n",
    "print(\"   ‚úÖ Expressions with equals sign (bill_apples = 3)\")\n",
    "print(\"   ‚úÖ Arithmetic operations (+, -, *, /)\")\n",
    "print(\"   ‚úÖ Variables and numbers\")\n",
    "print(\"   ‚úÖ Parentheses for grouping\")\n",
    "print(\"\\n‚ùå This grammar blocks:\")\n",
    "print(\"   ‚ùå Natural language ('Bill has 3 apples')\")\n",
    "print(\"   ‚ùå Explanations ('The answer is 5 because...')\")\n",
    "print(\"   ‚ùå Anything that's not a math expression\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STEP 2: LOAD MODEL AND CREATE PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check if model is already loaded (from Example 2)\n",
    "try:\n",
    "    # Test if hf_model exists\n",
    "    _ = hf_model\n",
    "    _ = tokenizer\n",
    "    print(\"\\n‚úÖ Using model already loaded from Example 2\")\n",
    "except NameError:\n",
    "    # Model not loaded, load it now\n",
    "    print(\"\\n‚è≥ Model not found, loading Qwen model...\")\n",
    "    MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=hf_token)\n",
    "    hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        token=hf_token\n",
    "    )\n",
    "    print(\"‚úÖ Model loaded successfully!\")\n",
    "\n",
    "# Create a text generation pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=hf_model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Pipeline created\")\n",
    "\n",
    "# Create grammar constraint\n",
    "print(\"\\n‚è≥ Creating grammar constraint and logits processor...\")\n",
    "grammar = IncrementalGrammarConstraint(grammar_str, \"root\", pipe.tokenizer)\n",
    "grammar_processor = GrammarConstrainedLogitsProcessor(grammar)\n",
    "\n",
    "print(\"‚úÖ Grammar constraint created!\")\n",
    "print(\"\\nüí° How it works:\")\n",
    "print(\"   1. Grammar is compiled into a constraint object\")\n",
    "print(\"   2. GrammarConstrainedLogitsProcessor intercepts token generation\")\n",
    "print(\"   3. At each step, it sets logits to -inf for invalid tokens\")\n",
    "print(\"   4. Only valid tokens (per grammar) can be selected\")\n",
    "print(\"   5. Result: Output GUARANTEED to match grammar\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 1: DEFINE BNF GRAMMAR FOR MATH EXPRESSIONS\n",
      "================================================================================\n",
      "\n",
      "üìù Math Expression Grammar defined!\n",
      "\n",
      "üîí This grammar enforces:\n",
      "   ‚úÖ Expressions with equals sign (bill_apples = 3)\n",
      "   ‚úÖ Arithmetic operations (+, -, *, /)\n",
      "   ‚úÖ Variables and numbers\n",
      "   ‚úÖ Parentheses for grouping\n",
      "\n",
      "‚ùå This grammar blocks:\n",
      "   ‚ùå Natural language ('Bill has 3 apples')\n",
      "   ‚ùå Explanations ('The answer is 5 because...')\n",
      "   ‚ùå Anything that's not a math expression\n",
      "\n",
      "================================================================================\n",
      "STEP 2: LOAD MODEL AND CREATE PIPELINE\n",
      "================================================================================\n",
      "\n",
      "‚è≥ Model not found, loading Qwen model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3fd896380e324d1ab7d815508bcc95af"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "‚úÖ Pipeline created\n",
      "\n",
      "‚è≥ Creating grammar constraint and logits processor...\n",
      "‚úÖ Grammar constraint created!\n",
      "\n",
      "üí° How it works:\n",
      "   1. Grammar is compiled into a constraint object\n",
      "   2. GrammarConstrainedLogitsProcessor intercepts token generation\n",
      "   3. At each step, it sets logits to -inf for invalid tokens\n",
      "   4. Only valid tokens (per grammar) can be selected\n",
      "   5. Result: Output GUARANTEED to match grammar\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Test with Math Word Problem\n",
    "\n",
    "Let's test with a real math word problem and see how the grammar constraint ensures we get only mathematical expressions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T18:37:12.847893Z",
     "start_time": "2026-01-15T18:36:47.954155Z"
    }
   },
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STEP 3: TESTING WITH MATH WORD PROBLEM\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Math word problem\n",
    "math_question = \"\"\"Bill has 3 apples and 2 oranges.\n",
    "Mae has 2 apples and 4 oranges.\n",
    "How many apples do Bill and Mae have in total?\"\"\"\n",
    "\n",
    "print(f\"\\nüìù Math Problem:\")\n",
    "print(f\"   {math_question}\")\n",
    "\n",
    "# Create system prompt\n",
    "system_prompt = \"\"\"You are a math instructor. I will ask you a math question.\n",
    "Respond with the mathematical expression that can be used to solve the problem.\"\"\"\n",
    "\n",
    "# Combine into input message\n",
    "input_message = f\"{system_prompt}\\n\\nQuestion: {math_question}\\n\\nMath Expression:\\n\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATING WITH GRAMMAR CONSTRAINT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Generate WITH grammar constraint\n",
    "print(\"\\n‚è≥ Generating with GrammarConstrainedLogitsProcessor...\")\n",
    "results = pipe(\n",
    "    input_message,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False,\n",
    "    logits_processor=[grammar_processor]  # <-- Grammar constraint!\n",
    ")\n",
    "\n",
    "# Extract generated text\n",
    "generated_text = results[0]['generated_text']\n",
    "# Get only the new part (after the input)\n",
    "math_expression = generated_text[len(input_message):].strip()\n",
    "\n",
    "print(f\"\\n‚úÖ Generated Math Expression:\")\n",
    "print(\"=\" * 80)\n",
    "print(math_expression)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüéØ Analysis:\")\n",
    "if \"bill_apples\" in math_expression or \"apples\" in math_expression:\n",
    "    print(\"   ‚úÖ Uses variable names (bill_apples, mae_apples, etc.)\")\n",
    "if \"+\" in math_expression:\n",
    "    print(\"   ‚úÖ Contains arithmetic operations\")\n",
    "if \"=\" in math_expression:\n",
    "    print(\"   ‚úÖ Contains equals sign\")\n",
    "if len(math_expression.split()) < 20:  # Short response\n",
    "    print(\"   ‚úÖ Concise (no explanations)\")\n",
    "    \n",
    "print(\"\\nüí° Key Point:\")\n",
    "print(\"   The model COULD NOT generate explanations like:\")\n",
    "print(\"   ‚ùå 'Bill has 3 apples and Mae has 2, so...'\")\n",
    "print(\"   ‚ùå 'The answer is 5 because...'\")\n",
    "print(\"   ‚ùå 'To solve this, we add 3 + 2...'\")\n",
    "print(\"\\n   Because those tokens are PHYSICALLY BLOCKED by the grammar!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STEP 3: TESTING WITH MATH WORD PROBLEM\n",
      "================================================================================\n",
      "\n",
      "üìù Math Problem:\n",
      "   Bill has 3 apples and 2 oranges.\n",
      "Mae has 2 apples and 4 oranges.\n",
      "How many apples do Bill and Mae have in total?\n",
      "\n",
      "================================================================================\n",
      "GENERATING WITH GRAMMAR CONSTRAINT\n",
      "================================================================================\n",
      "\n",
      "‚è≥ Generating with GrammarConstrainedLogitsProcessor...\n",
      "\n",
      "‚úÖ Generated Math Expression:\n",
      "================================================================================\n",
      "3 +2\n",
      "+2 +4\n",
      "\n",
      "+3\n",
      "\n",
      "=13\n",
      "3 +2 +2 +4 = 13\n",
      "\n",
      "3 +2 = 5\n",
      "2 +4 = 6\n",
      "5 +6 = 11\n",
      "\n",
      "3 +2 +2 +4 = 11\n",
      "\n",
      "3 +2 = 5\n",
      "2 +4 = 6\n",
      "5 +6 = 11\n",
      "\n",
      "5 +6 = 11\n",
      "\n",
      "11\n",
      "\n",
      "=11\n",
      "\n",
      "3 +2 +2 +4 = 11\n",
      "\n",
      "11\n",
      "\n",
      "=11\n",
      "\n",
      "11\n",
      "\n",
      "=11\n",
      "\n",
      "11\n",
      "\n",
      "=11\n",
      "\n",
      "11\n",
      "\n",
      "=11\n",
      "\n",
      "11\n",
      "\n",
      "=11\n",
      "\n",
      "11\n",
      "\n",
      "=11\n",
      "\n",
      "11\n",
      "\n",
      "=11\n",
      "\n",
      "11\n",
      "\n",
      "=11\n",
      "\n",
      "11\n",
      "\n",
      "=11\n",
      "\n",
      "11\n",
      "\n",
      "=11\n",
      "\n",
      "11\n",
      "\n",
      "=11\n",
      "\n",
      "11\n",
      "\n",
      "=11\n",
      "\n",
      "11\n",
      "\n",
      "=11\n",
      "\n",
      "11\n",
      "\n",
      "=11\n",
      "\n",
      "11\n",
      "\n",
      "=11\n",
      "\n",
      "11\n",
      "\n",
      "=11\n",
      "\n",
      "11\n",
      "\n",
      "=11\n",
      "\n",
      "11\n",
      "\n",
      "=11\n",
      "\n",
      "11\n",
      "\n",
      "=11\n",
      "\n",
      "11\n",
      "\n",
      "=11\n",
      "\n",
      "11\n",
      "\n",
      "=1\n",
      "================================================================================\n",
      "\n",
      "üéØ Analysis:\n",
      "   ‚úÖ Contains arithmetic operations\n",
      "   ‚úÖ Contains equals sign\n",
      "\n",
      "üí° Key Point:\n",
      "   The model COULD NOT generate explanations like:\n",
      "   ‚ùå 'Bill has 3 apples and Mae has 2, so...'\n",
      "   ‚ùå 'The answer is 5 because...'\n",
      "   ‚ùå 'To solve this, we add 3 + 2...'\n",
      "\n",
      "   Because those tokens are PHYSICALLY BLOCKED by the grammar!\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Comparison - Without Grammar Constraint\n",
    "\n",
    "Let's see what happens when we DON'T use the grammar constraint."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"COMPARISON: WITHOUT GRAMMAR CONSTRAINT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n‚è≥ Generating WITHOUT GrammarConstrainedLogitsProcessor...\")\n",
    "\n",
    "# Generate WITHOUT grammar constraint\n",
    "results_no_grammar = pipe(\n",
    "    input_message,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False\n",
    "    # No logits_processor!\n",
    ")\n",
    "\n",
    "# Extract generated text\n",
    "generated_no_grammar = results_no_grammar[0]['generated_text']\n",
    "response_no_grammar = generated_no_grammar[len(input_message):].strip()\n",
    "\n",
    "print(f\"\\n‚ùå Generated Response (no constraint):\")\n",
    "print(\"=\" * 80)\n",
    "print(response_no_grammar[:500])  # Limit to first 500 chars\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Problems WITHOUT grammar constraint:\")\n",
    "print(\"   ‚Ä¢ May include natural language explanations\")\n",
    "print(\"   ‚Ä¢ May provide the answer instead of expression\")\n",
    "print(\"   ‚Ä¢ May include reasoning steps\")\n",
    "print(\"   ‚Ä¢ Unpredictable format\")\n",
    "print(\"   ‚Ä¢ Hard to parse programmatically\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéØ KEY INSIGHT: How Grammar Constraints Work\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nWhat outlines and llama-cpp do internally:\")\n",
    "print(\"   1. Compile grammar into a state machine\")\n",
    "print(\"   2. At each token generation step:\")\n",
    "print(\"      ‚Üí Check which tokens are valid per current state\")\n",
    "print(\"      ‚Üí Set logits of INVALID tokens to -inf\")\n",
    "print(\"      ‚Üí Model MUST choose from valid tokens\")\n",
    "print(\"   3. Result: 100% guaranteed format compliance\")\n",
    "\n",
    "print(\"\\nThis is the foundational mechanism!\")\n",
    "print(\"   ‚Ä¢ outlines: Abstracts this into Generator(model, regex(...))\")\n",
    "print(\"   ‚Ä¢ llama-cpp: Abstracts this into grammar=LlamaGrammar(...)\")\n",
    "print(\"   ‚Ä¢ This example: Shows the raw logits processing\")\n",
    "\n",
    "print(\"\\n‚úÖ Now you understand how grammar constraints work under the hood!\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}