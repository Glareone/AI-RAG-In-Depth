{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern 2. Grammar: 4 Battle-tested Examples\n",
    "## Notebook ideas\n",
    "\n",
    "1) compare Structured Output and True Grammar Pattern.\n",
    "2) demonstrate how to use True Grammar Pattern using llama-cpp and outlines.\n",
    "\n",
    "## Cases + Ideas\n",
    "* Guaranteed Format and Rule Compliance\n",
    "* Generate Proper SQL Query\n",
    "* Fix grammar in your input sentence (generating the correct one on output)\n",
    "* Understand the underlying rules (math, natural language, query language, etc)\n",
    "\n",
    "This notebook demonstrates the Grammar Pattern with 4 practical examples and touches the model from different angles:\n",
    "\n",
    "1. **Insurance Forms** - Complex nested JSON extraction\n",
    "2. **SQL Query Generation** - Generate valid SQL against real database using Grammar pattern and `outlines`\n",
    "3. **Pipe-Separated Data** - Extract structured data with strict format\n",
    "4. **English** - fix the grammar in your sentence\n",
    "\n",
    "**Key Learning:** Grammar Pattern provides 100% guarantee of valid output format\n",
    "\n",
    "## Grammar Pattern approaches. Outlines vs Llama-cpp\n",
    "\n",
    "| Feature | outlines | llama-cpp |\n",
    "|---------|----------|-----------|\n",
    "| Model Format | HuggingFace models | GGUF models |\n",
    "| Precision | FP16/BF16 (full) | Quantized (Q4, Q5, Q8) |\n",
    "| Model Download | ‚úÖ Auto-downloads | ‚ö†Ô∏è Manual download required |\n",
    "| Grammar Support | Regex, JSON schema, FSM | ‚úÖ Full BNF grammar |\n",
    "| Model Size | ~6GB (full precision) | ~2.5GB (Q5 quantization) |\n",
    "| Token-level Masking | ‚úÖ Yes | ‚úÖ Yes |\n",
    "| Format Guarantee | ‚úÖ 100% guaranteed | ‚úÖ 100% guaranteed |\n",
    "| Grammar Violation | ‚ùå Physically impossible | ‚ùå Physically impossible |\n",
    "| Library Ecosystem | transformers + outlines | llama-cpp-python |\n",
    "| Hardware Optimization | GPU/MPS preferred | CPU optimized |\n",
    "| Setup Complexity | Medium (pip install) | Medium (download + pip) |\n",
    "| Best For | Latest models, GPU | Smaller footprint, CPU |\n",
    "| Memory Usage | Higher (~6-8GB) | Lower (~3-4GB) |\n",
    "| Integration | Python native | C++ backend |\n",
    "\n",
    "## Structured output vs Grammar Pattern\n",
    "\n",
    "|  Feature | Grammar Pattern         | Structured Outputs                |\n",
    "|----------|-------------------------|-----------------------------------|\n",
    "|          | (outlines)              | (Azure OpenAI)                    |\n",
    "| Implementation           | Self-hosted model       | Azure OpenAI API                  |\n",
    "| Constraint Type          | Token-level grammar     | Schema + parsing                  |\n",
    " Safety Guarantee         | ‚úÖ HARD (impossible)     | ‚ö†Ô∏è  SOFT (almost impossible, 99%) |\n",
    "| Grammar Support          | ‚úÖ BNF, regex, FSM       | ‚ùå Not supported                   |\n",
    "| Dangerous SQL Blocking   | ‚úÖ Physically blocked    | ‚ö†Ô∏è  Relies on prompt              |\n",
    "| Output Structure         | ‚ö†Ô∏è  Text (grammar)      | ‚úÖ Pydantic objects                |\n",
    "| Setup Complexity         | Medium (install)        | Low (just API)                    |\n",
    "| Runtime Performance      | Local (fast)            | API call (latency)                |\n",
    "| Cost                     | Hardware only           | Per-token pricing                 |\n",
    "| Model Control            | ‚úÖ Full control          | ‚ùå Server-side only                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install openai pydantic python-dotenv transformers torch accelerate pandas outlines\n",
    "\n",
    "# Optional: For llama-cpp alternative (requires manual model download)\n",
    "# !pip install llama-cpp-python\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T20:38:06.718964Z",
     "start_time": "2026-01-12T20:38:06.464010Z"
    }
   },
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Literal\n",
    "from enum import Enum\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API key is set\n",
    "if not os.getenv(\"AZURE_OPENAI_API_KEY\") or not os.getenv(\"AZURE_OPENAI_ENDPOINT\") or not os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"):\n",
    "    raise ValueError(\"Please set AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT_NAME environment variable\")\n",
    "\n",
    "# Setup Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "# gpt-4o, 4o-mini, 4.1-mini, and others could be used with slightly different results\n",
    "MODEL = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Setup complete!\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Example 1: Insurance Forms - Complex Nested JSON Extraction\n",
    "## Uses approach named Text Generation Inference (TGI) or Structured Output\n",
    "### Structured Output: This approach works in OpenAI/Bedrock models using structured output\n",
    "### Text Generation Inference (TGI): It also works in llama-cpp models through TGI\n",
    "\n",
    "## Business Problem\n",
    "\n",
    "1. Different fields, complex nesting\n",
    "Insurance companies process thousands of claim forms daily. These forms contain:\n",
    "  - Personal information, field mapping.\n",
    "  - Multiple incidents (car accidents often have multiple vehicles)\n",
    "  - Nested damage descriptions\n",
    "  - Medical records\n",
    "  - Financial details\n",
    "\n",
    "2. Make the model less verbal in places where it's not necessary\n",
    "  - Do not put extra words, just generate requested content.\n",
    "\n",
    "**Challenge:** Parsing errors cause claim delays. Extra wording\n",
    "\n",
    "**Solution:** Grammar Pattern with Pydantic Schema guarantees valid structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Example 1. Step 1. Define Complex Insurance Schema"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Enums for controlled vocabularies\n",
    "class ClaimType(str, Enum):\n",
    "    AUTO = \"auto\"\n",
    "    HEALTH = \"health\"\n",
    "    HOME = \"home\"\n",
    "    LIFE = \"life\"\n",
    "\n",
    "class IncidentSeverity(str, Enum):\n",
    "    MINOR = \"minor\"\n",
    "    MODERATE = \"moderate\"\n",
    "    SEVERE = \"severe\"\n",
    "    TOTAL_LOSS = \"total_loss\"\n",
    "\n",
    "class InjuryType(str, Enum):\n",
    "    NONE = \"none\"\n",
    "    MINOR = \"minor\"\n",
    "    SERIOUS = \"serious\"\n",
    "    CRITICAL = \"critical\"\n",
    "\n",
    "# Nested structures - No @dataclass needed with BaseModel!\n",
    "class PersonalInfo(BaseModel):\n",
    "    full_name: str = Field(description=\"Full legal name\")\n",
    "    policy_number: str = Field(description=\"Insurance policy number\")\n",
    "    phone: str = Field(description=\"Contact phone number\")\n",
    "    email: Optional[str] = Field(default=None, description=\"Email address\")\n",
    "\n",
    "class Address(BaseModel):\n",
    "    street: str\n",
    "    city: str\n",
    "    state: str\n",
    "    zip_code: str\n",
    "\n",
    "class VehicleInfo(BaseModel):\n",
    "    license_plate: str\n",
    "    vin: Optional[str] = Field(default=None, description=\"Vehicle Identification Number\")\n",
    "    make: str = Field(description=\"Vehicle manufacturer\")\n",
    "    model: str = Field(description=\"Vehicle model\")\n",
    "    year: int = Field(ge=1900, le=2030, description=\"Model year\")\n",
    "\n",
    "class DamageItem(BaseModel):\n",
    "    component: str = Field(description=\"Damaged component (e.g., 'front bumper', 'windshield')\")\n",
    "    description: str = Field(description=\"Detailed damage description\")\n",
    "    estimated_cost: float = Field(ge=0, description=\"Estimated repair cost in USD\")\n",
    "\n",
    "class Injury(BaseModel):\n",
    "    person_name: str\n",
    "    injury_type: InjuryType\n",
    "    description: str\n",
    "    medical_facility: Optional[str] = Field(default=None, description=\"Hospital or clinic name\")\n",
    "\n",
    "class Incident(BaseModel):\n",
    "    incident_location: Address\n",
    "    severity: IncidentSeverity\n",
    "    weather_conditions: Optional[str] = Field(default=None)\n",
    "    description: str = Field(description=\"Detailed description of what happened\")\n",
    "    police_report_filed: bool = Field(default=False, description=\"Whether police report was filed\")\n",
    "    police_report_number: Optional[str] = Field(default=None, description=\"Police report number if filed\")\n",
    "    incident_date: str = Field(description=\"Date of the incident in the following format (YYYY-MM-DD)\")\n",
    "\n",
    "class AutoIncident(Incident):\n",
    "    vehicles_involved: List[VehicleInfo] = Field(min_length=1)\n",
    "    damages: List[DamageItem] = Field(description=\"List of damages to each vehicle\")\n",
    "    injuries: List[Injury] = Field(default=[], description=\"List of injuries\")\n",
    "    other_driver_info: Optional[PersonalInfo] = Field(default=None)\n",
    "\n",
    "# Main claim structure\n",
    "class InsuranceClaim(BaseModel):\n",
    "    claim_type: ClaimType\n",
    "    claimant: PersonalInfo\n",
    "    incident: AutoIncident\n",
    "    claim_id: str = Field(description=\"Unique claim identifier\")\n",
    "    filing_date: str = Field(description=\"Date claim was filed (YYYY-MM-DD)\")\n",
    "    total_estimated_cost: float = Field(ge=0, description=\"Total estimated cost in USD\")\n",
    "    priority: Literal[\"low\", \"medium\", \"high\", \"urgent\"] = Field(\n",
    "        default=\"medium\",\n",
    "        description=\"Claim priority level\"\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Complex insurance schema setup is finished!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1. Step 2. Sample Insurance Claim Form (Unstructured Text)\n",
    "The idea is to extract JSON-formatted output which fits our schema to be processed further"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "insurance_form_text = \"\"\"\n",
    "CLAIM REPORT - AUTO ACCIDENT\n",
    "\n",
    "Date Filed: January 5, 2026\n",
    "Claim Reference: CLM-2026-00472\n",
    "\n",
    "CLAIMANT INFORMATION:\n",
    "Name: Sarah Johnson\n",
    "Policy #: POL-847392-AZ\n",
    "Contact: (555) 123-4567\n",
    "Email: sarah.johnson@email.com\n",
    "\n",
    "INCIDENT DETAILS:\n",
    "Date of Accident: December 28, 2025\n",
    "Location: 1234 Main Street, Phoenix, Arizona, 85001\n",
    "Weather: Rainy conditions, reduced visibility\n",
    "Police Report: Yes, Report #PX-2025-9847\n",
    "\n",
    "DESCRIPTION:\n",
    "I was driving northbound on Main Street at approximately 3:30 PM when another vehicle \n",
    "ran a red light and struck my vehicle on the passenger side. The impact caused \n",
    "significant damage to both vehicles. The other driver admitted fault at the scene.\n",
    "\n",
    "MY VEHICLE:\n",
    "2023 Toyota Camry\n",
    "License Plate: ABC-1234\n",
    "VIN: 1HGBH41JXMN109186\n",
    "\n",
    "DAMAGES TO MY VEHICLE:\n",
    "1. Front passenger door - Major dent and paint damage - Estimated $2,500\n",
    "2. Rear passenger door - Moderate dent - Estimated $1,800\n",
    "3. Passenger side mirror - Broken, needs replacement - Estimated $450\n",
    "4. Front passenger window - Shattered - Estimated $350\n",
    "\n",
    "OTHER VEHICLE:\n",
    "2021 Honda Civic\n",
    "License Plate: XYZ-9876\n",
    "Driver: Michael Chen\n",
    "Driver's Policy: POL-293847-CA\n",
    "Driver's Phone: (555) 987-6543\n",
    "\n",
    "DAMAGES TO OTHER VEHICLE:\n",
    "1. Front bumper - Completely destroyed - Estimated $1,200\n",
    "2. Hood - Crumpled - Estimated $2,000\n",
    "3. Headlight assembly - Both broken - Estimated $800\n",
    "\n",
    "INJURIES:\n",
    "1. Sarah Johnson (me) - Minor whiplash and bruising - Treated at Phoenix General Hospital\n",
    "2. Passenger Emma Johnson (my daughter, age 8) - Minor cuts from broken glass - \n",
    "   Treated at Phoenix General Hospital\n",
    "\n",
    "SEVERITY ASSESSMENT: Moderate - vehicles drivable but require significant repairs\n",
    "\n",
    "TOTAL ESTIMATED DAMAGES: $9,100\n",
    "\n",
    "Priority: HIGH (injuries involved)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Sample Insurance Form Loaded\")\n",
    "print(\"=\" * 80)\n",
    "print(insurance_form_text[:500] + \"...\")\n",
    "print(\"\\nüìÑ This unstructured text needs to be parsed into structured InsuranceClaim object\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1. Step 3. \"Option 1, Pydantic Schema\".\n",
    "### Extract with Pydantic Schema and Grammar Pattern (Guaranteed Valid output)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def extract_insurance_claim(form_text: str) -> InsuranceClaim:\n",
    "    \"\"\"\n",
    "    Extract insurance claim with Grammar Pattern guarantee.\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"\n",
    "    You are an expert insurance claim processing system. Extract all relevant information from the claim form.\n",
    "    \n",
    "    ## Insurance Form Structure:\n",
    "    \n",
    "    The insurance claim form typically contains the following sections:\n",
    "    \n",
    "    ### 1. HEADER SECTION\n",
    "    - Claim ID/Reference number (e.g., \"CLM-2026-00472\")\n",
    "    - Filing date (when the claim was submitted)\n",
    "    \n",
    "    ### 2. CLAIMANT INFORMATION SECTION\n",
    "    - Full legal name of the person filing the claim\n",
    "    - Policy number (format: POL-XXXXXX-XX)\n",
    "    - Contact phone number (format: (XXX) XXX-XXXX)\n",
    "    - Email address (optional)\n",
    "    \n",
    "    ### 3. INCIDENT DETAILS SECTION\n",
    "    - Date of accident/incident (YYYY-MM-DD format)\n",
    "    - Location: full address including street, city, state, and zip code\n",
    "    - Weather conditions at time of incident (optional)\n",
    "    - Police report information (whether filed, report number if available)\n",
    "    - Detailed narrative description of what happened\n",
    "    \n",
    "    ### 4. VEHICLES INVOLVED (for auto claims)\n",
    "    Each vehicle section includes:\n",
    "    - Year, Make, Model (e.g., \"2023 Toyota Camry\")\n",
    "    - License plate number\n",
    "    - VIN (Vehicle Identification Number) - optional\n",
    "    - For other vehicles: driver name, driver's policy number, contact info\n",
    "    \n",
    "    ### 5. DAMAGES SECTION\n",
    "    List of damaged components for each vehicle:\n",
    "    - Component name (e.g., \"front bumper\", \"passenger door\", \"windshield\")\n",
    "    - Detailed description of the damage\n",
    "    - Estimated repair cost in USD (numeric value)\n",
    "    \n",
    "    ### 6. INJURIES SECTION (if applicable)\n",
    "    For each injured person:\n",
    "    - Person's name\n",
    "    - Type of injury (none, minor, serious, critical)\n",
    "    - Description of injuries\n",
    "    - Medical facility where treated (hospital/clinic name) - optional\n",
    "    \n",
    "    ### 7. ASSESSMENT SECTION\n",
    "    - Severity classification (minor, moderate, severe, total_loss)\n",
    "    - Total estimated damages (sum of all repair costs)\n",
    "    - Priority level (low, medium, high, urgent) - often based on injuries and severity\n",
    "    \n",
    "    ## Extraction Rules:\n",
    "    \n",
    "    1. **Be thorough**: Extract ALL vehicles mentioned, even if it's the other driver's vehicle\n",
    "    2. **Accuracy**: Use exact values from the form - don't approximate or round numbers\n",
    "    3. **Dates**: Convert all dates to YYYY-MM-DD format\n",
    "    4. **Costs**: Extract numeric values only, convert to float (remove $, commas)\n",
    "    5. **Enums**: Map text to correct enum values:\n",
    "       - Claim type: auto, health, home, life\n",
    "       - Severity: minor, moderate, severe, total_loss\n",
    "       - Injury type: none, minor, serious, critical\n",
    "    6. **Missing data**: Use appropriate defaults:\n",
    "       - Optional fields can be null\n",
    "       - Use empty lists [] for injuries if none reported\n",
    "    7. **Priority**: Infer from context:\n",
    "       - urgent: life-threatening injuries or total loss\n",
    "       - high: any injuries or severe damage\n",
    "       - medium: moderate damage, no injuries\n",
    "       - low: minor damage only\n",
    "    \n",
    "    ## Common Patterns to Watch For:\n",
    "    \n",
    "    - \"MY VEHICLE\" vs \"OTHER VEHICLE\" sections - both should be included\n",
    "    - Damage estimates may be listed per item or as subtotals\n",
    "    - Police report: \"Yes\" means filed=true, extract the report number\n",
    "    - Multiple people may be injured in a single incident\n",
    "    - The claimant's vehicle should be listed first in vehicles_involved\n",
    "    \n",
    "    Extract with precision and completeness.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": form_text}\n",
    "        ],\n",
    "        response_format=InsuranceClaim,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.parsed\n",
    "\n",
    "print(\"Extracting insurance claim with enhanced system prompt...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "claim = extract_insurance_claim(insurance_form_text)\n",
    "\n",
    "print(\"\\n‚úÖ SUCCESSFULLY EXTRACTED!\\n\")\n",
    "print(f\"Claim ID: {claim.claim_id}\")\n",
    "print(f\"Type: {claim.claim_type.value}\")\n",
    "print(f\"Priority: {claim.priority}\")\n",
    "print(f\"\\nClaimant: {claim.claimant.full_name}\")\n",
    "print(f\"Policy: {claim.claimant.policy_number}\")\n",
    "print(f\"\\nIncident Date: {claim.incident.incident_date}\")\n",
    "print(f\"Location: {claim.incident.incident_location.street}, {claim.incident.incident_location.city}\")\n",
    "print(f\"Severity: {claim.incident.severity.value}\")\n",
    "print(f\"Police Report: {'Yes' if claim.incident.police_report_filed else 'No'}\")\n",
    "\n",
    "print(f\"\\nVehicles Involved: {len(claim.incident.vehicles_involved)}\")\n",
    "for i, vehicle in enumerate(claim.incident.vehicles_involved, 1):\n",
    "    print(f\"  {i}. {vehicle.year} {vehicle.make} {vehicle.model} ({vehicle.license_plate})\")\n",
    "\n",
    "print(f\"\\nDamages: {len(claim.incident.damages)}\")\n",
    "for i, damage in enumerate(claim.incident.damages, 1):\n",
    "    print(f\"  {i}. {damage.component}: ${damage.estimated_cost:,.2f}\")\n",
    "\n",
    "print(f\"\\nInjuries: {len(claim.incident.injuries)}\")\n",
    "for i, injury in enumerate(claim.incident.injuries, 1):\n",
    "    print(f\"  {i}. {injury.person_name}: {injury.injury_type.value} - {injury.description[:50]}...\")\n",
    "\n",
    "print(f\"\\nTotal Estimated Cost: ${claim.total_estimated_cost:,.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüéØ ENHANCED SYSTEM PROMPT BENEFITS:\")\n",
    "print(\"  ‚úÖ Explains insurance form structure in detail\")\n",
    "print(\"  ‚úÖ Provides clear extraction rules\")\n",
    "print(\"  ‚úÖ Guides on handling edge cases (MY VEHICLE vs OTHER VEHICLE)\")\n",
    "print(\"  ‚úÖ Specifies date and number formatting\")\n",
    "print(\"  ‚úÖ Maps common phrases to enum values\")\n",
    "print(\"  ‚úÖ Improves accuracy with context-specific instructions\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1. Step 3. \"Option 2: Json-Object\".\n",
    "### Compare: JSON Mode With Pydantic Classes + Grammar Pattern\n",
    "in order to use Json Mode you have to specify response format this way:\n",
    "\n",
    "`response_format={\"type\": \"json_object\"}`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Simulate WITHOUT grammar pattern (just JSON mode)\n",
    "def extract_without_grammar(form_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract without schema constraint - just asks for JSON.\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"\n",
    "    Extract insurance claim information and return it in JSON format.\n",
    "    Include: claim_id, claimant info, vehicles, damages, injuries, costs.\n",
    "    Return valid JSON only.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": form_text}\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"Extracting WITHOUT grammar pattern...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "json_result = extract_without_grammar(insurance_form_text)\n",
    "print(\"\\nRaw JSON output:\")\n",
    "print(json.dumps(json.loads(json_result), indent=2))\n",
    "\n",
    "# Try to parse into our schema\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n‚ö†Ô∏è PROBLEMS WITHOUT GRAMMAR PATTERN:\")\n",
    "try:\n",
    "    parsed_dict = json.loads(json_result)\n",
    "    print(\"‚úÖ Valid JSON\")\n",
    "    \n",
    "    # But check if it matches our schema\n",
    "    try:\n",
    "        claim_obj = InsuranceClaim(**parsed_dict)\n",
    "        print(\"‚úÖ Matches InsuranceClaim schema (lucky!)\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Does NOT match InsuranceClaim schema!\")\n",
    "        print(f\"   Error: {str(e)[:200]}...\")\n",
    "        \n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"‚ùå Invalid JSON: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Example 1. Step 4. Export to JSON for downstream systems"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Export the claim\n",
    "claim_json = claim.model_dump_json(indent=2)\n",
    "\n",
    "print(\"Final Structured Claim (ready for downstream systems):\")\n",
    "print(\"=\" * 80)\n",
    "print(claim_json)\n",
    "\n",
    "# Save to file\n",
    "with open('insurance_claim.json', 'w') as f:\n",
    "    f.write(claim_json)\n",
    "\n",
    "print(\"\\n‚úÖ Saved to insurance_claim.json\")\n",
    "print(\"\\nüéØ This JSON is GUARANTEED to be valid and processable!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Example 2: SQL Query Generation with TRUE BNF Grammar Pattern\n",
    "\n",
    "## Business Problem\n",
    "\n",
    "1. Generate Safe queries to fetch natural data using user's instructions.\n",
    "2. Natural language to SQL system should generate syntactically correct queries:\n",
    "- No Wrong column names\n",
    "- Only valid SQL syntax\n",
    "- Not Malformed WHERE clauses\n",
    "- No dangerous operations. **DANGEROUS: DELETE, UPDATE, DROP operations from malicious/confused prompts**\n",
    "\n",
    "**Solution:** Use **TRUE BNF Grammar Pattern** with llama-cpp to guarantee safe, valid SQL.\n",
    "\n",
    "## What We'll Demonstrate\n",
    "\n",
    "Unlike Example 1 (Structured Outputs with Azure OpenAI), this example uses:\n",
    "- **Self-hosted model** (Qwen 2.5-3B in GGUF format)\n",
    "- **llama-cpp-python** library\n",
    "- **BNF Grammar** as a hard constraint (using outlines for Grammar)\n",
    "\n",
    "This is the **REAL Grammar Pattern** from the article you mentioned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create Sample Database & Define BNF Grammar\n",
    "\n",
    "We'll create an in-memory SQLite database and define the BNF grammar for safe SQL."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================================\n",
    "# Part A: Create Sample Database\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"PART A: CREATING SAMPLE DATABASE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create in-memory SQLite database\n",
    "conn = sqlite3.connect(':memory:')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create tables\n",
    "cursor.execute('''\n",
    "CREATE TABLE employees (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    name TEXT NOT NULL,\n",
    "    department TEXT NOT NULL,\n",
    "    salary REAL NOT NULL,\n",
    "    hire_date TEXT NOT NULL,\n",
    "    manager_id INTEGER\n",
    ")\n",
    "''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE departments (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    name TEXT NOT NULL,\n",
    "    budget REAL NOT NULL,\n",
    "    location TEXT NOT NULL\n",
    ")\n",
    "''')\n",
    "\n",
    "# Insert sample data\n",
    "employees_data = [\n",
    "    (1, 'John Smith', 'Engineering', 95000, '2020-01-15', None),\n",
    "    (2, 'Sarah Johnson', 'Engineering', 87000, '2021-03-20', 1),\n",
    "    (3, 'Michael Chen', 'Engineering', 82000, '2022-06-10', 1),\n",
    "    (4, 'Emily Brown', 'Sales', 78000, '2020-08-01', None),\n",
    "    (5, 'David Lee', 'Sales', 71000, '2021-11-15', 4),\n",
    "    (6, 'Maria Garcia', 'Sales', 69000, '2023-02-01', 4),\n",
    "    (7, 'James Wilson', 'Marketing', 76000, '2021-05-10', None),\n",
    "    (8, 'Lisa Anderson', 'Marketing', 68000, '2022-09-20', 7),\n",
    "    (9, 'Robert Taylor', 'HR', 72000, '2020-04-15', None),\n",
    "    (10, 'Jennifer Martinez', 'HR', 65000, '2023-01-10', 9)\n",
    "]\n",
    "\n",
    "cursor.executemany('INSERT INTO employees VALUES (?, ?, ?, ?, ?, ?)', employees_data)\n",
    "\n",
    "departments_data = [\n",
    "    (1, 'Engineering', 500000, 'San Francisco'),\n",
    "    (2, 'Sales', 300000, 'New York'),\n",
    "    (3, 'Marketing', 250000, 'Los Angeles'),\n",
    "    (4, 'HR', 150000, 'Chicago')\n",
    "]\n",
    "\n",
    "cursor.executemany('INSERT INTO departments VALUES (?, ?, ?, ?)', departments_data)\n",
    "conn.commit()\n",
    "\n",
    "print(\"‚úÖ Sample database created!\")\n",
    "print(\"\\nTables:\")\n",
    "print(\"  ‚Ä¢ employees (id, name, department, salary, hire_date, manager_id)\")\n",
    "print(\"  ‚Ä¢ departments (id, name, budget, location)\")\n",
    "\n",
    "print(\"\\nSample data:\")\n",
    "print(pd.read_sql_query(\"SELECT * FROM employees LIMIT 3\", conn))\n",
    "print(\"\\n\", pd.read_sql_query(\"SELECT * FROM departments\", conn))\n",
    "\n",
    "# ============================================================================\n",
    "# Part B: Define BNF Grammar for Safe SQL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PART B: DEFINING BNF GRAMMAR FOR SAFE SQL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# BNF Grammar for Safe SQL (SELECT only)\n",
    "SQL_BNF_GRAMMAR = \"\"\"\n",
    "root ::= ws select_statement ws\n",
    "\n",
    "select_statement ::= \"SELECT\" ws select_list ws from_clause (ws where_clause)? (ws group_by_clause)? (ws order_by_clause)? (ws limit_clause)? \";\"?\n",
    "\n",
    "select_list ::= \"*\" | column_list\n",
    "column_list ::= column_expr (ws \",\" ws column_expr)*\n",
    "column_expr ::= (identifier \".\")? identifier (ws \"AS\" ws identifier)?\n",
    "            | aggregate_func \"(\" ws (column_expr | \"*\") ws \")\" (ws \"AS\" ws identifier)?\n",
    "\n",
    "aggregate_func ::= \"COUNT\" | \"SUM\" | \"AVG\" | \"MIN\" | \"MAX\"\n",
    "\n",
    "from_clause ::= \"FROM\" ws table_ref (ws join_clause)*\n",
    "table_ref ::= identifier (ws \"AS\"? ws identifier)?\n",
    "\n",
    "join_clause ::= join_type ws \"JOIN\" ws table_ref ws \"ON\" ws condition\n",
    "join_type ::= \"INNER\" | \"LEFT\" | \"RIGHT\" | \"FULL\"\n",
    "\n",
    "where_clause ::= \"WHERE\" ws condition\n",
    "\n",
    "condition ::= simple_condition (ws logic_op ws simple_condition)*\n",
    "simple_condition ::= column_expr ws comparison_op ws value\n",
    "                  | column_expr ws \"BETWEEN\" ws value ws \"AND\" ws value\n",
    "                  | column_expr ws \"IN\" ws \"(\" ws value_list ws \")\"\n",
    "                  | \"(\" ws condition ws \")\"\n",
    "\n",
    "comparison_op ::= \"=\" | \"!=\" | \"<\" | \">\" | \"<=\" | \">=\"\n",
    "logic_op ::= \"AND\" | \"OR\"\n",
    "\n",
    "group_by_clause ::= \"GROUP BY\" ws column_list (ws \"HAVING\" ws condition)?\n",
    "\n",
    "order_by_clause ::= \"ORDER BY\" ws order_expr (ws \",\" ws order_expr)*\n",
    "order_expr ::= column_expr (ws (\"ASC\" | \"DESC\"))?\n",
    "\n",
    "limit_clause ::= \"LIMIT\" ws number (ws \"OFFSET\" ws number)?\n",
    "\n",
    "value ::= number | string_literal | identifier\n",
    "value_list ::= value (ws \",\" ws value)*\n",
    "string_literal ::= \"'\" [^']* \"'\"\n",
    "\n",
    "identifier ::= [a-zA-Z_][a-zA-Z0-9_]*\n",
    "number ::= [0-9]+ (\".\" [0-9]+)?\n",
    "\n",
    "ws ::= [ \\t\\n]*\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nüìù BNF Grammar defined!\")\n",
    "print(\"\\nüîí What this grammar enforces:\")\n",
    "print(\"   ‚úÖ root ::= ws select_statement ws\")\n",
    "print(\"      ‚Üí ONLY SELECT statements allowed\")\n",
    "print(\"      ‚Üí DELETE, UPDATE, INSERT, DROP are NOT in the grammar\")\n",
    "print(\"      ‚Üí Model CANNOT generate these tokens (logit = -inf)\")\n",
    "print(\"\\n   ‚úÖ Supports:\")\n",
    "print(\"      ‚Ä¢ WHERE clauses with conditions\")\n",
    "print(\"      ‚Ä¢ JOINs (INNER, LEFT, RIGHT, FULL)\")\n",
    "print(\"      ‚Ä¢ GROUP BY with HAVING\")\n",
    "print(\"      ‚Ä¢ ORDER BY with ASC/DESC\")\n",
    "print(\"      ‚Ä¢ LIMIT and OFFSET\")\n",
    "print(\"      ‚Ä¢ Aggregate functions (COUNT, SUM, AVG, MIN, MAX)\")\n",
    "print(\"\\n   ‚úÖ Blocks:\")\n",
    "print(\"      ‚Ä¢ DELETE (not in grammar)\")\n",
    "print(\"      ‚Ä¢ UPDATE (not in grammar)\")\n",
    "print(\"      ‚Ä¢ INSERT (not in grammar)\")\n",
    "print(\"      ‚Ä¢ DROP, TRUNCATE, ALTER, CREATE (not in grammar)\")\n",
    "\n",
    "print(\"\\nüí° Key Insight:\")\n",
    "print(\"   Since 'DELETE', 'UPDATE', etc. don't appear anywhere in the BNF,\")\n",
    "print(\"   llama-cpp will set their token probabilities to -inf (impossible).\")\n",
    "print(\"   This is TRUE token-level enforcement!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Database and Grammar are ready!\")\n",
    "print(\"=\" * 80)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Self-Hosted Model which supports grammar\n",
    "\n",
    "Now we'll load the model and use TRUE Grammar Pattern with BNF constraints."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T20:39:14.601373Z",
     "start_time": "2026-01-12T20:39:09.702395Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from outlines import from_transformers\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOADING MODEL WITH OUTLINES + HUGGINGFACE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìö Using 'outlines' library for TRUE Grammar Pattern\")\n",
    "print(\"   ‚Ä¢ Supports BNF grammar, regex, and JSON schema constraints\")\n",
    "print(\"   ‚Ä¢ Token-level logits masking based on grammar rules\")\n",
    "print(\"   ‚Ä¢ Same approach used by HuggingFace TGI\")\n",
    "print(\"   ‚Ä¢ Works directly with HuggingFace transformers\")\n",
    "\n",
    "# Get HuggingFace token from environment (optional for non-gated models)\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "print(f\"\\nLoading model: {MODEL_NAME}...\")\n",
    "print(\"This may take a few minutes on first run (downloads model)...\\n\")\n",
    "\n",
    "# Step 1: Load HuggingFace tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=hf_token)\n",
    "\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,  # Use bfloat16 for efficiency\n",
    "    device_map=\"auto\",  # Automatically use GPU/MPS if available\n",
    "    token=hf_token\n",
    ")\n",
    "\n",
    "print(\"‚úÖ HuggingFace model loaded\")\n",
    "\n",
    "# Step 2: Wrap with outlines for grammar support\n",
    "print(\"\\n‚è≥ Wrapping model with outlines...\")\n",
    "model = from_transformers(hf_model, tokenizer)\n",
    "\n",
    "print(\"\\n‚úÖ Model loaded successfully!\")\n",
    "print(f\"   Model: {MODEL_NAME}\")\n",
    "print(f\"   Device: {hf_model.device}\")\n",
    "print(f\"   dtype: {hf_model.dtype}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Now we can use TRUE Grammar constraints!\")\n",
    "print(\"=\" * 80)\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alekseikolesnikov/miniconda3/envs/deeplearning-ai/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING MODEL WITH OUTLINES + HUGGINGFACE\n",
      "================================================================================\n",
      "\n",
      "üìö Using 'outlines' library for TRUE Grammar Pattern\n",
      "   ‚Ä¢ Supports BNF grammar, regex, and JSON schema constraints\n",
      "   ‚Ä¢ Token-level logits masking based on grammar rules\n",
      "   ‚Ä¢ Same approach used by HuggingFace TGI\n",
      "   ‚Ä¢ Works directly with HuggingFace transformers\n",
      "\n",
      "Loading model: Qwen/Qwen2.5-3B-Instruct...\n",
      "This may take a few minutes on first run (downloads model)...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5dfb1eede7764af0b5f9569812db1da9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ HuggingFace model loaded\n",
      "\n",
      "‚è≥ Wrapping model with outlines...\n",
      "\n",
      "‚úÖ Model loaded successfully!\n",
      "   Model: Qwen/Qwen2.5-3B-Instruct\n",
      "   Device: mps:0\n",
      "   dtype: torch.bfloat16\n",
      "\n",
      "================================================================================\n",
      "Now we can use TRUE Grammar constraints!\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Test 1. Generate SQL with TRUE BNF Grammar Constraint\n",
    "Now we'll use the BNF grammar to generate SQL queries with HARD constraints."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from outlines import Generator, regex\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CREATING SQL GRAMMAR WITH OUTLINES (REGEX)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüîß Grammar Type: Regex (Regular Expression)\")\n",
    "print(\"   ‚Ä¢ Constrains output to match specific patterns\")\n",
    "print(\"   ‚Ä¢ Token-level logits masking (tokens that don't match = -inf)\")\n",
    "print(\"   ‚Ä¢ TRUE Grammar Pattern (not just prompt engineering)\")\n",
    "\n",
    "# Define SQL regex pattern that only allows SELECT statements\n",
    "sql_regex = r\"SELECT[\\s\\S]+FROM[\\s\\S]+\"\n",
    "\n",
    "print(\"\\nüìù SQL Regex Pattern:\")\n",
    "print(f\"   {sql_regex}\")\n",
    "\n",
    "print(\"\\nüîí This regex enforces:\")\n",
    "print(\"   ‚úÖ Must start with SELECT\")\n",
    "print(\"   ‚úÖ Must have FROM clause\")\n",
    "print(\"   ‚ùå IMPOSSIBLE: DELETE, UPDATE, INSERT, DROP, ALTER\")\n",
    "print(\"      (These keywords are not in the regex, so tokens are blocked)\")\n",
    "\n",
    "# Create generator with regex grammar\n",
    "print(\"\\n‚è≥ Compiling regex grammar...\")\n",
    "sql_generator = Generator(model, output_type=regex(sql_regex))\n",
    "print(\"‚úÖ Grammar compiled!\")\n",
    "\n",
    "print(\"\\nüí° How it works:\")\n",
    "print(\"   1. outlines compiles regex to finite state machine (FSM)\")\n",
    "print(\"   2. At each token generation:\")\n",
    "print(\"      ‚Üí FSM determines which tokens are valid\")\n",
    "print(\"      ‚Üí Invalid tokens get logit = -inf (impossible to generate)\")\n",
    "print(\"      ‚Üí Model MUST choose valid token\")\n",
    "print(\"   3. Output GUARANTEED to match regex\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TESTING: Generate SQL Queries with Grammar Constraint\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test questions - SAFE requests\n",
    "test_questions = [\n",
    "    \"Show me all employees in Engineering department\",\n",
    "    \"What is the average salary by department?\",\n",
    "    \"List employees who earn more than $75,000\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"Generate a SQL SELECT query.\n",
    "\n",
    "Database Schema:\n",
    "- employees: id, name, department, salary, hire_date, manager_id\n",
    "- departments: id, name, budget, location\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "SQL Query:\\n\"\"\"\n",
    "    \n",
    "    print(\"\\n‚è≥ Generating with regex grammar constraint...\")\n",
    "    \n",
    "    try:\n",
    "        # Generate with grammar constraint\n",
    "        sql_query = sql_generator(prompt)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Generated SQL:\")\n",
    "        print(f\"   {sql_query}\")\n",
    "        \n",
    "        # Try to execute it\n",
    "        try:\n",
    "            # Clean the query\n",
    "            clean_sql = sql_query.split(';')[0].strip()\n",
    "            if not clean_sql.upper().startswith('SELECT'):\n",
    "                clean_sql = 'SELECT' + clean_sql\n",
    "                \n",
    "            df = pd.read_sql_query(clean_sql, conn)\n",
    "            print(f\"\\nüìä Execution: SUCCESS\")\n",
    "            print(f\"   Rows returned: {len(df)}\")\n",
    "            if len(df) > 0:\n",
    "                print(f\"\\n   Results preview:\")\n",
    "                print(df.head().to_string(index=False))\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è Execution error: {str(e)[:150]}\")\n",
    "            print(\"   Note: Grammar ensures SELECT-only, but not perfect SQL syntax\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Generation failed: {str(e)[:200]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Test 2. SQL Query generation Test. BNF Grammar Block Dangerous Operations?\n",
    "\n",
    "Let's try to make the model generate dangerous SQL operations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TESTING GRAMMAR SAFETY: Attempting Dangerous Operations\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Dangerous requests that should be blocked\n",
    "dangerous_questions = [\n",
    "    \"Delete all employees from the Engineering department\",\n",
    "    \"Update all salaries to $100,000\",\n",
    "    \"Drop the employees table\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(dangerous_questions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"‚ö†Ô∏è Dangerous Request {i}: {question}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    prompt = f\"\"\"Generate SQL for this request.\n",
    "\n",
    "Database Schema:\n",
    "- employees: id, name, department, salary, hire_date, manager_id\n",
    "- departments: id, name, budget, location\n",
    "\n",
    "Request: {question}\n",
    "\n",
    "SQL Query:\\n\"\"\"\n",
    "    \n",
    "    print(\"\\n‚è≥ Attempting to generate with grammar constraint...\")\n",
    "    \n",
    "    try:\n",
    "        # Generate with grammar constraint\n",
    "        # The regex ONLY allows SELECT, so dangerous operations are impossible\n",
    "        generated_sql = sql_generator(prompt)\n",
    "        \n",
    "        print(f\"\\nüìù What the model generated:\")\n",
    "        print(f\"   {generated_sql}\")\n",
    "        \n",
    "        # Check if it's a SELECT\n",
    "        if generated_sql.strip().upper().startswith(\"SELECT\"):\n",
    "            print(f\"\\n‚úÖ SAFE: Grammar forced a SELECT query instead!\")\n",
    "            print(f\"   The model CANNOT generate DELETE/UPDATE/DROP with this regex.\")\n",
    "            print(f\"   Even when explicitly asked, grammar physically blocks it.\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è Unexpected: Not a SELECT query\")\n",
    "            print(f\"   This shouldn't happen with the regex constraint.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Generation failed: {str(e)[:150]}\")\n",
    "        print(\"   This might happen if the model can't satisfy the grammar\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üõ°Ô∏è GRAMMAR PATTERN SAFETY DEMONSTRATED:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n‚úÖ Dangerous SQL operations were BLOCKED by grammar constraint\")\n",
    "print(\"   ‚Ä¢ DELETE, UPDATE, INSERT, DROP are NOT in the regex pattern\")\n",
    "print(\"   ‚Ä¢ Tokens for these keywords get logit = -inf (impossible)\")\n",
    "print(\"   ‚Ä¢ Model physically CANNOT generate them\")\n",
    "print(\"   ‚Ä¢ This is TRUE Grammar Pattern - hard constraint at token level\")\n",
    "\n",
    "print(\"\\nüéØ Key Difference from Prompt Engineering:\")\n",
    "print(\"   ‚ùå Prompt: 'Only generate SELECT' ‚Üí Model might ignore\")\n",
    "print(\"   ‚úÖ Grammar: regex only allows SELECT ‚Üí Physically impossible to violate\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Alternative: Using llama-cpp-python with BNF Grammar\n",
    "\n",
    "The `llama-cpp-python` library provides an alternative way to use TRUE BNF Grammar Pattern.\n",
    "\n",
    "**Key Differences:**\n",
    "- `outlines`: Works with HuggingFace models (FP16/BF16), auto-downloads\n",
    "- `llama-cpp`: Works with GGUF models (quantized), requires manual download\n",
    "\n",
    "**Both provide TRUE Grammar Pattern** with token-level logits masking.\n",
    "\n",
    "### When to use llama-cpp:\n",
    "- Need smaller model size (quantized GGUF)\n",
    "- Want faster inference on CPU\n",
    "- Prefer BNF grammar over regex\n",
    "\n",
    "### Setup:\n",
    "```bash\n",
    "# Install llama-cpp-python\n",
    "pip install llama-cpp-python\n",
    "\n",
    "# Download GGUF model (example)\n",
    "# Visit: https://huggingface.co/Qwen/Qwen2.5-3B-Instruct-GGUF\n",
    "# Download: qwen2.5-3b-instruct-q5_k_m.gguf (~2.5GB)\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install llama-cpp-python",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# OPTIONAL: llama-cpp-python implementation\n# Uncomment to use this alternative approach\n\nfrom llama_cpp import Llama, LlamaGrammar\nfrom huggingface_hub import hf_hub_download\nimport os\n\nprint(\"=\" * 80)\nprint(\"LOADING MODEL WITH LLAMA-CPP + BNF GRAMMAR\")\nprint(\"=\" * 80)\n\n# Create models directory if it doesn't exist\nos.makedirs(\"models\", exist_ok=True)\n\n# Model details\nREPO_ID = \"Qwen/Qwen2.5-3B-Instruct-GGUF\"\nFILENAME = \"qwen2.5-3b-instruct-q5_k_m.gguf\"\nMODEL_PATH = f\"models/{FILENAME}\"\n\n# Download if not exists\nif not os.path.exists(MODEL_PATH):\n    print(f\"\\nüì• Downloading {FILENAME} (~2.5GB)...\")\n    print(\"   This may take 5-10 minutes depending on your connection...\")\n    MODEL_PATH = hf_hub_download(\n        repo_id=REPO_ID,\n        filename=FILENAME,\n        local_dir=\"models\",\n        local_dir_use_symlinks=False\n    )\n    print(\"‚úÖ Download complete!\")\nelse:\n    print(f\"\\n‚úÖ Model already exists at {MODEL_PATH}\")\n\nprint(f\"\\nüì¶ Loading model from: {MODEL_PATH}\")\nprint(\"   This may take 30-60 seconds...\\n\")\n\n# Load model with llama-cpp\nllm = Llama(\n    model_path=MODEL_PATH,\n    n_ctx=2048,           # Context window\n    n_threads=8,          # Use multiple threads\n    n_gpu_layers=0,       # 0 = CPU only, increase for Metal/MPS\n    verbose=False\n)\n    \nprint(\"‚úÖ Model loaded successfully!\")\n    \n# Create BNF grammar\nprint(\"\\n‚è≥ Compiling BNF grammar...\")\nsql_grammar = LlamaGrammar.from_string(SQL_BNF_GRAMMAR)\nprint(\"‚úÖ Grammar compiled!\")\n    \n# Test generation\nprint(\"\\n\" + \"=\" * 80)\nprint(\"TESTING: Generate SQL with llama-cpp BNF Grammar\")\nprint(\"=\" * 80)\n    \ntest_question = \"What is the average salary by department?\"\n    \nprompt = f\"\"\"Generate a SQL SELECT query.\nDatabase Schema:\n- employees: id, name, department, salary, hire_date, manager_id\n- departments: id, name, budget, location\n\nQuestion: {test_question}\n\nSQL Query:\\n\"\"\"\n    \nprint(f\"\\nQuestion: {test_question}\")\nprint(\"\\n‚è≥ Generating with BNF grammar constraint...\")\n    \n# Generate with grammar constraint\nresponse = llm(\n    prompt,\n    max_tokens=150,\n    temperature=0.3,\n   grammar=sql_grammar,  # <-- BNF Grammar constraint!\n    stop=[\";\", \"\\n\\n\"]\n)\n    \nsql_query = response['choices'][0]['text'].strip()\n    \nprint(f\"\\n‚úÖ Generated SQL:\")\nprint(f\"   {sql_query}\")\n    \n# Try to execute\ntry:\n    df = pd.read_sql_query(sql_query, conn)\n    print(f\"\\nüìä Execution: SUCCESS\")\n    print(f\"   Rows returned: {len(df)}\")\n    if len(df) > 0:\n        print(f\"\\n   Results:\")\n        print(df.to_string(index=False))\nexcept Exception as e:\n    print(f\"\\n‚ö†Ô∏è Execution error: {str(e)[:150]}\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"‚úÖ Both outlines and llama-cpp provide TRUE Grammar Pattern!\")\nprint(\"   See comparison table below for detailed differences.\")\nprint(\"=\" * 80)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Implementation Comparison: outlines vs llama-cpp\n\nBoth libraries provide TRUE Grammar Pattern with token-level logits masking.\n\n| Feature | outlines | llama-cpp |\n|---------|----------|-----------|\n| Model Format | HuggingFace models | GGUF models |\n| Precision | FP16/BF16 (full) | Quantized (Q4, Q5, Q8) |\n| Model Download | ‚úÖ Auto-downloads | ‚ö†Ô∏è Manual download required |\n| Grammar Support | Regex, JSON schema, FSM | ‚úÖ Full BNF grammar |\n| Model Size | ~6GB (full precision) | ~2.5GB (Q5 quantization) |\n| Token-level Masking | ‚úÖ Yes | ‚úÖ Yes |\n| Format Guarantee | ‚úÖ 100% guaranteed | ‚úÖ 100% guaranteed |\n| Grammar Violation | ‚ùå Physically impossible | ‚ùå Physically impossible |\n| Library Ecosystem | transformers + outlines | llama-cpp-python |\n| Hardware Optimization | GPU/MPS preferred | CPU optimized |\n| Setup Complexity | Medium (pip install) | Medium (download + pip) |\n| Best For | Latest models, GPU | Smaller footprint, CPU |\n| Memory Usage | Higher (~6-8GB) | Lower (~3-4GB) |\n| Integration | Python native | C++ backend |\n\n**Bottom Line:** Both provide TRUE Grammar Pattern. Choose based on your infrastructure:\n- **outlines**: When you want latest HuggingFace models with auto-download\n- **llama-cpp**: When you need smaller model size and CPU efficiency",
   "metadata": {}
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "## Example 3. Pipe-separated extraction\n",
    "\n",
    "Extract Named Entities from the text in the following format:\n",
    "`SKU | Product Name | Price | Category`\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T20:36:26.340749Z",
     "start_time": "2026-01-12T20:36:26.335636Z"
    }
   },
   "source": [
    "# Sample product descriptions\n",
    "product_descriptions = [\n",
    "    \"\"\"\n",
    "    Apple iPhone 17 Pro - Latest flagship smartphone with Pro chip,\n",
    "    48MP camera system, and titanium design. SKU: IPHONE-17-PRO-256.\n",
    "    Price: $999. Category: Electronics/Smartphones\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Samsung 65\" QLED 4K Smart TV - Quantum HDR, Object Tracking Sound+, \n",
    "    and Gaming Hub. Model: QN65Q80C. Retail price: $1,299.99\n",
    "    Category: Electronics/TVs\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Nike Air Max 270 - Men's running shoes with visible Max Air cushioning.\n",
    "    Style code: DM9652-001. Price $160. Category: Footwear/Athletic\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Organic Green Tea - Premium loose leaf tea from Japan. \n",
    "    No SKU assigned yet. Price: $24.99 per package. Category: Groceries/Beverages\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "print(\"Sample Product Descriptions:\")\n",
    "print(\"=\" * 80)\n",
    "for i, desc in enumerate(product_descriptions, 1):\n",
    "    print(f\"\\n{i}. {desc.strip()[:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüéØ Goal: Extract as pipe-separated format:\")\n",
    "print(\"   SKU | Product Name | Price | Category\")\n",
    "print(\"\\nConstraints:\")\n",
    "print(\"  ‚Ä¢ Exactly 3 pipes (4 fields)\")\n",
    "print(\"  ‚Ä¢ Use NULL for missing SKU\")\n",
    "print(\"  ‚Ä¢ Only alphanumeric and basic punctuation\")\n",
    "print(\"  ‚Ä¢ Price must be numeric\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Product Descriptions:\n",
      "================================================================================\n",
      "\n",
      "1. Apple iPhone 17 Pro - Latest flagship smartphone with Pro chip,\n",
      "    48MP camera system, and titanium...\n",
      "\n",
      "2. Samsung 65\" QLED 4K Smart TV - Quantum HDR, Object Tracking Sound+, \n",
      "    and Gaming Hub. Model: QN65...\n",
      "\n",
      "3. Nike Air Max 270 - Men's running shoes with visible Max Air cushioning.\n",
      "    Style code: DM9652-001. ...\n",
      "\n",
      "4. Organic Green Tea - Premium loose leaf tea from Japan. \n",
      "    No SKU assigned yet. Price: $24.99 per p...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üéØ Goal: Extract as pipe-separated format:\n",
      "   SKU | Product Name | Price | Category\n",
      "\n",
      "Constraints:\n",
      "  ‚Ä¢ Exactly 3 pipes (4 fields)\n",
      "  ‚Ä¢ Use NULL for missing SKU\n",
      "  ‚Ä¢ Only alphanumeric and basic punctuation\n",
      "  ‚Ä¢ Price must be numeric\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Pipe-Separated Schema\n",
    "\n",
    "For pipe-separated format, we'll use Pydantic schema approach (simpler than BNF for this case)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T20:36:59.838526Z",
     "start_time": "2026-01-12T20:36:59.790730Z"
    }
   },
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class ProductRecord(BaseModel):\n",
    "    \"\"\"Single product in structured format (we'll convert to pipe-separated)\"\"\"\n",
    "    sku: str = Field(description=\"Product SKU, use 'NULL' if not available\")\n",
    "    product_name: str = Field(description=\"Product name, alphanumeric only\")\n",
    "    price: float = Field(ge=0, description=\"Product price in USD\")\n",
    "    category: str = Field(description=\"Product category\")\n",
    "    \n",
    "    def to_pipe_format(self) -> str:\n",
    "        \"\"\"Convert to pipe-separated format\"\"\"\n",
    "        return f\"{self.sku}|{self.product_name}|{self.price:.2f}|{self.category}\"\n",
    "\n",
    "def extract_product_record(description: str) -> ProductRecord:\n",
    "    \"\"\"\n",
    "    Extract product information with Grammar Pattern.\n",
    "    \"\"\"\n",
    "    system_prompt = \"\"\"\n",
    "    Extract product information from the description.\n",
    "    Use 'NULL' for SKU if not provided.\n",
    "    Clean product name to alphanumeric characters only.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.beta.chat.completions.parse(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": description}\n",
    "        ],\n",
    "        response_format=ProductRecord,\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.parsed\n",
    "\n",
    "print(\"‚úÖ Pipe-separated extraction ready!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipe-separated extraction ready!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Products in Pipe-Separated Format"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T20:38:20.888621Z",
     "start_time": "2026-01-12T20:38:17.233956Z"
    }
   },
   "source": [
    "print(\"Extracting Products in Pipe-Separated Format\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "pipe_records = []\n",
    "\n",
    "for i, description in enumerate(product_descriptions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"\\nüì¶ Product {i}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(description.strip()[:150] + \"...\")\n",
    "    \n",
    "    # Extract with Grammar Pattern\n",
    "    product = extract_product_record(description)\n",
    "    \n",
    "    # Convert to pipe format\n",
    "    pipe_format = product.to_pipe_format()\n",
    "    pipe_records.append(pipe_format)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Extracted (structured):\")\n",
    "    print(f\"   SKU: {product.sku}\")\n",
    "    print(f\"   Name: {product.product_name}\")\n",
    "    print(f\"   Price: ${product.price:.2f}\")\n",
    "    print(f\"   Category: {product.category}\")\n",
    "    \n",
    "    print(f\"\\nüìÑ Pipe-separated format:\")\n",
    "    print(f\"   {pipe_format}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüìã FINAL OUTPUT (ready for legacy system import):\")\n",
    "print(\"=\" * 80)\n",
    "for record in pipe_records:\n",
    "    print(record)\n",
    "\n",
    "print(\"\\nüéØ GUARANTEES:\")\n",
    "print(\"  ‚úÖ Exactly 3 pipes in each line\")\n",
    "print(\"  ‚úÖ Price always numeric (float)\")\n",
    "print(\"  ‚úÖ NULL used for missing SKU\")\n",
    "print(\"  ‚úÖ No parsing errors\")\n",
    "print(\"  ‚úÖ Ready for direct import to legacy system\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Products in Pipe-Separated Format\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üì¶ Product 1:\n",
      "--------------------------------------------------------------------------------\n",
      "Apple iPhone 17 Pro - Latest flagship smartphone with Pro chip,\n",
      "    48MP camera system, and titanium design. SKU: IPHONE-17-PRO-256.\n",
      "    Price: $999. ...\n",
      "\n",
      "‚úÖ Extracted (structured):\n",
      "   SKU: IPHONE-17-PRO-256\n",
      "   Name: Apple iPhone 17 Pro\n",
      "   Price: $999.00\n",
      "   Category: Electronics/Smartphones\n",
      "\n",
      "üìÑ Pipe-separated format:\n",
      "   IPHONE-17-PRO-256|Apple iPhone 17 Pro|999.00|Electronics/Smartphones\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üì¶ Product 2:\n",
      "--------------------------------------------------------------------------------\n",
      "Samsung 65\" QLED 4K Smart TV - Quantum HDR, Object Tracking Sound+, \n",
      "    and Gaming Hub. Model: QN65Q80C. Retail price: $1,299.99\n",
      "    Category: Electr...\n",
      "\n",
      "‚úÖ Extracted (structured):\n",
      "   SKU: QN65Q80C\n",
      "   Name: Samsung65QLED4KSmartTV\n",
      "   Price: $1299.99\n",
      "   Category: Electronics/TVs\n",
      "\n",
      "üìÑ Pipe-separated format:\n",
      "   QN65Q80C|Samsung65QLED4KSmartTV|1299.99|Electronics/TVs\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üì¶ Product 3:\n",
      "--------------------------------------------------------------------------------\n",
      "Nike Air Max 270 - Men's running shoes with visible Max Air cushioning.\n",
      "    Style code: DM9652-001. Price $160. Category: Footwear/Athletic...\n",
      "\n",
      "‚úÖ Extracted (structured):\n",
      "   SKU: DM9652-001\n",
      "   Name: NikeAirMax270MensrunningshoeswithvisibleMaxAircushioning\n",
      "   Price: $160.00\n",
      "   Category: Footwear/Athletic\n",
      "\n",
      "üìÑ Pipe-separated format:\n",
      "   DM9652-001|NikeAirMax270MensrunningshoeswithvisibleMaxAircushioning|160.00|Footwear/Athletic\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üì¶ Product 4:\n",
      "--------------------------------------------------------------------------------\n",
      "Organic Green Tea - Premium loose leaf tea from Japan. \n",
      "    No SKU assigned yet. Price: $24.99 per package. Category: Groceries/Beverages...\n",
      "\n",
      "‚úÖ Extracted (structured):\n",
      "   SKU: NULL\n",
      "   Name: OrganicGreenTea\n",
      "   Price: $24.99\n",
      "   Category: Groceries/Beverages\n",
      "\n",
      "üìÑ Pipe-separated format:\n",
      "   NULL|OrganicGreenTea|24.99|Groceries/Beverages\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìã FINAL OUTPUT (ready for legacy system import):\n",
      "================================================================================\n",
      "IPHONE-17-PRO-256|Apple iPhone 17 Pro|999.00|Electronics/Smartphones\n",
      "QN65Q80C|Samsung65QLED4KSmartTV|1299.99|Electronics/TVs\n",
      "DM9652-001|NikeAirMax270MensrunningshoeswithvisibleMaxAircushioning|160.00|Footwear/Athletic\n",
      "NULL|OrganicGreenTea|24.99|Groceries/Beverages\n",
      "\n",
      "üéØ GUARANTEES:\n",
      "  ‚úÖ Exactly 3 pipes in each line\n",
      "  ‚úÖ Price always numeric (float)\n",
      "  ‚úÖ NULL used for missing SKU\n",
      "  ‚úÖ No parsing errors\n",
      "  ‚úÖ Ready for direct import to legacy system\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to File"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save to PSV (Pipe-Separated Values) file\n",
    "with open('products.psv', 'w') as f:\n",
    "    f.write(\"SKU|Product Name|Price|Category\\n\")  # Header\n",
    "    for record in pipe_records:\n",
    "        f.write(record + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ Saved to products.psv\")\n",
    "print(\"\\nFile contents:\")\n",
    "with open('products.psv', 'r') as f:\n",
    "    print(f.read())\n",
    "\n",
    "print(\"\\nüéØ This file is GUARANTEED to be:\")\n",
    "print(\"  ‚úÖ Properly formatted\")\n",
    "print(\"  ‚úÖ Importable by legacy systems\")\n",
    "print(\"  ‚úÖ No manual validation needed\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Example 4: English Grammar Correction\n",
    "\n",
    "## Business Problem\n",
    "\n",
    "Grammar correction tools need to:\n",
    "- Fix grammatical errors in user input\n",
    "- Preserve the original meaning\n",
    "- Output ONLY the corrected sentence (not explanations)\n",
    "- Handle various types of errors (subject-verb agreement, tense, articles, etc.)\n",
    "\n",
    "**Challenge:** LLMs naturally want to explain their corrections or add commentary.\n",
    "\n",
    "**Solution:** Grammar Pattern constrains output to be valid English sentence only.\n",
    "\n",
    "## What We'll Demonstrate\n",
    "\n",
    "We'll use **regex grammar** with `outlines` to ensure:\n",
    "1. Output is a single sentence\n",
    "2. Starts with capital letter\n",
    "3. Ends with proper punctuation\n",
    "4. Contains only valid characters (no explanations, no bullet points)\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define English Sentence Grammar\n",
    "\n",
    "We'll use regex to constrain the output to valid English sentences:\n",
    "\n",
    "```regex\n",
    "^[A-Z][A-Za-z0-9\\s,.'\"\\-!?]+[.!?]$\n",
    "```\n",
    "\n",
    "**What this regex enforces:**\n",
    "- `^[A-Z]` - Must start with capital letter\n",
    "- `[A-Za-z0-9\\s,.'\"\\-!?]+` - Contains letters, numbers, spaces, and basic punctuation\n",
    "- `[.!?]$` - Must end with sentence-ending punctuation\n",
    "\n",
    "**What it prevents:**\n",
    "- Multi-sentence responses\n",
    "- Explanations (e.g., \"The error was...\")\n",
    "- Bullet points or formatting\n",
    "- Missing capitalization or punctuation\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from outlines import Generator, regex\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CREATING ENGLISH SENTENCE GRAMMAR\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Regex for valid English sentence\n",
    "# - Starts with capital letter\n",
    "# - Contains alphanumeric, spaces, and basic punctuation\n",
    "# - Ends with . ! or ?\n",
    "english_sentence_regex = r\"[A-Z][A-Za-z0-9\\s,.'\\\"\\-!?]*[.!?]\"\n",
    "\n",
    "print(\"\\nüìù English Sentence Regex:\")\n",
    "print(f\"   {english_sentence_regex}\")\n",
    "\n",
    "print(\"\\nüîí This regex enforces:\")\n",
    "print(\"   ‚úÖ Starts with capital letter\")\n",
    "print(\"   ‚úÖ Valid characters only (letters, numbers, basic punctuation)\")\n",
    "print(\"   ‚úÖ Ends with proper punctuation (. ! ?)\")\n",
    "print(\"   ‚ùå BLOCKS: Multi-sentence responses, explanations, formatting\")\n",
    "\n",
    "print(\"\\n‚è≥ Creating grammar-constrained generator...\")\n",
    "grammar_corrector = Generator(model, output_type=regex(english_sentence_regex))\n",
    "print(\"‚úÖ Grammar generator created!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T20:39:19.634018Z",
     "start_time": "2026-01-12T20:39:19.302428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING ENGLISH SENTENCE GRAMMAR\n",
      "================================================================================\n",
      "\n",
      "üìù English Sentence Regex:\n",
      "   [A-Z][A-Za-z0-9\\s,.'\\\"\\-!?]*[.!?]\n",
      "\n",
      "üîí This regex enforces:\n",
      "   ‚úÖ Starts with capital letter\n",
      "   ‚úÖ Valid characters only (letters, numbers, basic punctuation)\n",
      "   ‚úÖ Ends with proper punctuation (. ! ?)\n",
      "   ‚ùå BLOCKS: Multi-sentence responses, explanations, formatting\n",
      "\n",
      "‚è≥ Creating grammar-constrained generator...\n",
      "‚úÖ Grammar generator created!\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test: Grammar Correction with Constrained Output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Sample sentences with grammatical errors\n",
    "test_sentences = [\n",
    "    \"She don't like pizza.\",  # Subject-verb agreement\n",
    "    \"He go to school yesterday.\",  # Tense error\n",
    "    \"I have a apple and orange.\",  # Article error\n",
    "    \"They was happy about the news.\",  # Subject-verb agreement\n",
    "    \"Me and him went to store.\",  # Pronoun case error\n",
    "    \"The cat it is sleeping on the chair.\",  # Redundant pronoun\n",
    "    \"She have three brother and two sister.\",  # Multiple errors\n",
    "]"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T20:39:24.918680Z",
     "start_time": "2026-01-12T20:39:24.916276Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"CORRECTING GRAMMAR WITH CONSTRAINED OUTPUT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, incorrect_sentence in enumerate(test_sentences, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Example {i}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"‚ùå Original: {incorrect_sentence}\")\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"Fix the grammar in this sentence. Output ONLY the corrected sentence, nothing else.\n",
    "\n",
    "Incorrect: {incorrect_sentence}\n",
    "\n",
    "Corrected: \"\"\"\n",
    "    \n",
    "    print(\"\\n‚è≥ Generating with grammar constraint...\")\n",
    "    \n",
    "    try:\n",
    "        # Generate with grammar constraint\n",
    "        corrected = grammar_corrector(prompt)\n",
    "        \n",
    "        print(f\"‚úÖ Corrected: {corrected}\")\n",
    "        \n",
    "        # Verify it matches our regex\n",
    "        import re\n",
    "        if re.match(english_sentence_regex, corrected):\n",
    "            print(\"   ‚úÖ Matches grammar constraints\")\n",
    "        else:\n",
    "            print(\"   ‚ö†Ô∏è  Does not match regex (shouldn't happen!)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Generation failed: {str(e)[:150]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéØ GRAMMAR PATTERN BENEFITS:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n‚úÖ Output is ALWAYS a valid sentence:\")\n",
    "print(\"   ‚Ä¢ Starts with capital letter\")\n",
    "print(\"   ‚Ä¢ Ends with proper punctuation\")\n",
    "print(\"   ‚Ä¢ No extra explanations or commentary\")\n",
    "print(\"   ‚Ä¢ Single sentence only (no paragraphs)\")\n",
    "print(\"\\n‚úÖ Physical constraint at token level:\")\n",
    "print(\"   ‚Ä¢ Model CANNOT generate invalid characters\")\n",
    "print(\"   ‚Ä¢ Model CANNOT skip capitalization or punctuation\")\n",
    "print(\"   ‚Ä¢ 100% guarantee of format compliance\")\n"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-12T20:39:33.255484Z",
     "start_time": "2026-01-12T20:39:26.586738Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CORRECTING GRAMMAR WITH CONSTRAINED OUTPUT\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Example 1:\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Original: She don't like pizza.\n",
      "\n",
      "‚è≥ Generating with grammar constraint...\n",
      "‚úÖ Corrected: She doesn't like pizza. She doesn't like pizza. Great! Here's the corrected sentence.\n",
      "   ‚úÖ Matches grammar constraints\n",
      "\n",
      "================================================================================\n",
      "Example 2:\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Original: He go to school yesterday.\n",
      "\n",
      "‚è≥ Generating with grammar constraint...\n",
      "‚úÖ Corrected: He went to school yesterday. To correct the sentence, \"He go to school yesterday,\" we need\n",
      "   ‚úÖ Matches grammar constraints\n",
      "\n",
      "================================================================================\n",
      "Example 3:\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Original: I have a apple and orange.\n",
      "\n",
      "‚è≥ Generating with grammar constraint...\n",
      "‚úÖ Corrected: Correction needed. The corrected sentence is missing. Could you please provide the correct version? I have an\n",
      "   ‚úÖ Matches grammar constraints\n",
      "\n",
      "================================================================================\n",
      "Example 4:\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Original: They was happy about the news.\n",
      "\n",
      "‚è≥ Generating with grammar constraint...\n",
      "‚úÖ Corrected: They were happy about the news. To correct the sentence, replace \"was\" with \"were\"\n",
      "   ‚úÖ Matches grammar constraints\n",
      "\n",
      "================================================================================\n",
      "Example 5:\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Original: Me and him went to store.\n",
      "\n",
      "‚è≥ Generating with grammar constraint...\n",
      "‚úÖ Corrected: Me and him went to the store. To fix the grammar, we changed \"store\" to \"\n",
      "   ‚úÖ Matches grammar constraints\n",
      "\n",
      "================================================================================\n",
      "Example 6:\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Original: The cat it is sleeping on the chair.\n",
      "\n",
      "‚è≥ Generating with grammar constraint...\n",
      "‚úÖ Corrected: The cat is sleeping on the chair. To correct the given sentence, we need to remove \"it\n",
      "   ‚úÖ Matches grammar constraints\n",
      "\n",
      "================================================================================\n",
      "Example 7:\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Original: She have three brother and two sister.\n",
      "\n",
      "‚è≥ Generating with grammar constraint...\n",
      "‚úÖ Corrected: She has three brothers and two sisters. To correct the given sentence, I will change \"have\"\n",
      "   ‚úÖ Matches grammar constraints\n",
      "\n",
      "================================================================================\n",
      "üéØ GRAMMAR PATTERN BENEFITS:\n",
      "================================================================================\n",
      "\n",
      "‚úÖ Output is ALWAYS a valid sentence:\n",
      "   ‚Ä¢ Starts with capital letter\n",
      "   ‚Ä¢ Ends with proper punctuation\n",
      "   ‚Ä¢ No extra explanations or commentary\n",
      "   ‚Ä¢ Single sentence only (no paragraphs)\n",
      "\n",
      "‚úÖ Physical constraint at token level:\n",
      "   ‚Ä¢ Model CANNOT generate invalid characters\n",
      "   ‚Ä¢ Model CANNOT skip capitalization or punctuation\n",
      "   ‚Ä¢ 100% guarantee of format compliance\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_sentence = \"She don't like pizza.\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARISON: Grammar Pattern vs Regular Prompting\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTest sentence: {test_sentence}\")\n",
    "\n",
    "# Without Grammar Pattern (using API)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚ùå WITHOUT GRAMMAR PATTERN (regular prompting):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "response_no_grammar = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a grammar checker. Fix errors in user sentences.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Fix the grammar: {test_sentence}\"}\n",
    "    ],\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(response_no_grammar.choices[0].message.content)\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è Problems:\")\n",
    "print(\"   ‚Ä¢ May include explanation ('The error is...')\")\n",
    "print(\"   ‚Ä¢ May format with quotes or markdown\")\n",
    "print(\"   ‚Ä¢ May include multiple variations\")\n",
    "print(\"   ‚Ä¢ Inconsistent output format\")\n",
    "print(\"   ‚Ä¢ Hard to parse programmatically\")\n",
    "\n",
    "# With Grammar Pattern\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ WITH GRAMMAR PATTERN (regex constraint):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "prompt = f\"\"\"Fix the grammar in this sentence. Output ONLY the corrected sentence.\n",
    "\n",
    "Incorrect: {test_sentence}\n",
    "\n",
    "Corrected: \"\"\"\n",
    "\n",
    "corrected = grammar_corrector(prompt)\n",
    "print(corrected)\n",
    "\n",
    "print(\"\\n‚úÖ Benefits:\")\n",
    "print(\"   ‚Ä¢ Always just the corrected sentence\")\n",
    "print(\"   ‚Ä¢ No explanations or formatting\")\n",
    "print(\"   ‚Ä¢ Consistent output structure\")\n",
    "print(\"   ‚Ä¢ Easy to parse and use\")\n",
    "print(\"   ‚Ä¢ Physical guarantee (not just prompt)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üí° KEY INSIGHT:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nRegular prompting (soft constraint):\")\n",
    "print(\"   ‚ö†Ô∏è  'Please only output the sentence' ‚Üí Model might ignore\")\n",
    "print(\"   ‚ö†Ô∏è  Relies on model's instruction following\")\n",
    "print(\"   ‚ö†Ô∏è  Can be bypassed or misunderstood\")\n",
    "print(\"\\nGrammar Pattern (hard constraint):\")\n",
    "print(\"   ‚úÖ Regex physically blocks invalid tokens\")\n",
    "print(\"   ‚úÖ Model CANNOT generate explanations\")\n",
    "print(\"   ‚úÖ 100% guaranteed format compliance\")\n",
    "print(\"\\nüéØ Use Grammar Pattern when output format MUST be exact!\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
