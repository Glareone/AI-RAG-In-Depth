{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True Logits Masking with Llama (Open-Source Models)\n",
    "## Using custom LogitsProcessor for complex business rules\n",
    "\n",
    "This notebook demonstrates **true logits masking** with full control over token generation.\n",
    "\n",
    "**Key difference from Notebook 1:**\n",
    "- ‚úÖ Direct access to logits array\n",
    "- ‚úÖ Can implement ANY custom logic\n",
    "- ‚úÖ Context-aware decisions\n",
    "- ‚úÖ Dynamic rules based on generation state\n",
    "- ‚úÖ No API limitations\n",
    "\n",
    "**Use case:** Medical or Legal advice generator with strict compliance rules\n",
    "\n",
    "**REQUIRES:** .env file to run and `HF_TOKEN`. To obtain the token visit the `https://huggingface.co/`\n",
    "\n",
    "# PS. Dont forget to stop the jupyter notebook server.\n",
    "## To stop: `jupyter notebook stop 8888`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "# Note: This requires GPU for reasonable performance\n",
    "!pip install transformers torch accelerate sentencepiece charset-normalizer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer, \n",
    "    LogitsProcessor,\n",
    "    LogitsProcessorList\n",
    ")\n",
    "import re\n",
    "from typing import List\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables (for HF_TOKEN)\n",
    "load_dotenv()\n",
    "\n",
    "# Get HuggingFace token from environment\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "if not hf_token:\n",
    "    raise ValueError(\"HF_TOKEN not found in .env file. Please add your Hugging Face token.\")\n",
    "\n",
    "\n",
    "# Check device availability - prioritize MPS for Apple Silicon\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # Apple Silicon GPU acceleration\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == \"cpu\":\n",
    "    print(\"‚ö†Ô∏è Warning: Running on CPU will be slow. GPU recommended.\")\n",
    "elif device == \"mps\":\n",
    "    print(\"‚úÖ Using Apple Silicon GPU acceleration (MPS)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer\n",
    "\n",
    "We'll use Llama-3-8B-Instruct. You can also use smaller models like:\n",
    "- `meta-llama/Llama-3.2-1B-Instruct` (smaller, faster)\n",
    "- `meta-llama/Llama-3.2-3B-Instruct` (medium)\n",
    "\n",
    "**Note:** You may need to accept the license on HuggingFace and set HF_TOKEN."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Choose your model - Updated with non-gated alternatives\n# MODEL_NAME = \"microsoft/Phi-3.5-mini-instruct\"  # 3.8B - RECOMMENDED, no access needed\nMODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\"  # 3B - Good quality, no access needed\n# MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"  # Smallest/fastest (requires access)\n# MODEL_NAME = \"meta-llama/Llama-3.2-3B-Instruct\"  # Good balance (requires access)\n# MODEL_NAME = \"meta-llama/Llama-3-8B-Instruct\"  # Best quality (requires access)\n\n# Get HuggingFace token from environment (optional for non-gated models)\nhf_token = os.getenv(\"HF_TOKEN\")\n\nprint(f\"Loading model: {MODEL_NAME}...\")\nprint(\"This may take a few minutes on first run (downloads model)...\\n\")\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=hf_token)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    dtype=torch.bfloat16,  # Use bfloat16 for efficiency\n    device_map=\"auto\",  # Automatically use GPU/MPS if available\n    token=hf_token\n)\n\nprint(f\"‚úÖ Model loaded successfully!\")\nprint(f\"   Model parameters: {model.num_parameters() / 1e9:.1f}B\")\nprint(f\"   Vocabulary size: {len(tokenizer)}\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case: Medical Advice Generator with Compliance Rules\n",
    "\n",
    "We'll create a medical advice chatbot that MUST follow strict compliance rules:\n",
    "\n",
    "### Rules:\n",
    "1. **Never claim certainty** - Ban words like \"definitely\", \"guaranteed\", \"always\"\n",
    "2. **No diagnoses** - Ban \"you have\", \"diagnosed with\"\n",
    "3. **Include disclaimers** - If discussing medication, must mention consulting doctor\n",
    "4. **Professional language only** - Ban informal words in medical context\n",
    "\n",
    "These rules are **context-aware** and **dynamic** - impossible with simple logit_bias!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation: Custom LogitsProcessor\n",
    "\n",
    "This is where the magic happens - we have FULL control over the logits!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "class MedicalComplianceLogitsProcessor(LogitsProcessor):\n    \"\"\"\n    Enforces medical compliance rules by masking tokens.\n    \n    This demonstrates TRUE logits masking:\n    - Direct access to logits\n    - Context-aware rules\n    - Dynamic behavior based on generated text\n    \"\"\"\n    \n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n        \n        # Rule 1: Banned certainty words\n        self.certainty_words = [\n            \"definitely\", \"guaranteed\", \"always\", \"never\", \n            \"certainly\", \"absolutely\", \"undoubtedly\"\n        ]\n        \n        # Rule 2: Banned diagnostic phrases (will check context)\n        self.diagnostic_phrases = [\n            \"you have\", \"diagnosed with\", \"you are suffering\"\n        ]\n        \n        # Rule 3: Informal words banned in medical context\n        self.informal_words = [\n            \"gonna\", \"wanna\", \"yeah\", \"nope\", \"kinda\", \"sorta\"\n        ]\n        \n        # Pre-compute token IDs for efficiency - handle ALL tokenization variants\n        self.banned_token_ids = set()\n        for word in self.certainty_words + self.informal_words:\n            # Ban all possible tokenizations:\n            # 1. With leading space: \" always\"\n            # 2. Without leading space: \"always\"\n            # 3. Capitalized with space: \" Always\"\n            # 4. Capitalized without space: \"Always\"\n            variants = [\n                \" \" + word,           # \" always\"\n                word,                 # \"always\"\n                \" \" + word.capitalize(),  # \" Always\"\n                word.capitalize()     # \"Always\"\n            ]\n            \n            for variant in variants:\n                tokens = self.tokenizer.encode(variant, add_special_tokens=False)\n                self.banned_token_ids.update(tokens)\n        \n        print(f\"   Banned {len(self.banned_token_ids)} token IDs across all variants\")\n    \n    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n        \"\"\"\n        This is called at EVERY token generation step.\n        \n        Args:\n            input_ids: Previously generated tokens [batch_size, seq_len]\n            scores: Raw logits for next token [batch_size, vocab_size]\n        \n        Returns:\n            Modified scores with banned tokens set to -inf\n        \"\"\"\n        \n        # Decode the text generated so far\n        generated_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n        \n        # Rule 1: Always ban certainty words (unconditional)\n        for token_id in self.banned_token_ids:\n            scores[:, token_id] = float('-inf')\n        \n        # Rule 2: Context-aware - ban diagnostic language\n        # Check if we're about to complete a diagnostic phrase\n        recent_text = generated_text[-50:].lower()  # Last 50 chars\n        \n        if \"you\" in recent_text:\n            # Ban tokens that would complete \"you have\", \"you are\"\n            have_tokens = self.tokenizer.encode(\" have\", add_special_tokens=False)\n            are_tokens = self.tokenizer.encode(\" are\", add_special_tokens=False)\n            for token_id in have_tokens + are_tokens:\n                scores[:, token_id] = float('-inf')\n        \n        # Rule 3: Dynamic - if discussing medication, require disclaimer\n        medication_keywords = [\"medication\", \"medicine\", \"drug\", \"prescription\"]\n        mentions_medication = any(keyword in generated_text.lower() for keyword in medication_keywords)\n        mentions_doctor = any(word in generated_text.lower() for word in [\"doctor\", \"physician\", \"healthcare provider\"])\n        \n        if mentions_medication and not mentions_doctor:\n            # If we've mentioned medication but not doctor consultation,\n            # strongly bias towards disclaimer-related tokens\n            consult_tokens = self.tokenizer.encode(\" consult\", add_special_tokens=False)\n            doctor_tokens = self.tokenizer.encode(\" doctor\", add_special_tokens=False)\n            \n            for token_id in consult_tokens + doctor_tokens:\n                scores[:, token_id] += 5.0  # Boost these tokens\n        \n        return scores\n\nprint(\"‚úÖ Custom LogitsProcessor defined!\")\nprint(\"\\nThis processor:\")\nprint(\"  1. Bans certainty words in ALL tokenization variants (always, Always, etc.)\")\nprint(\"  2. Prevents diagnostic language (context-aware)\")\nprint(\"  3. Promotes disclaimer if medication mentioned (dynamic)\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function: Generate with Compliance"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def generate_medical_advice(question: str, use_masking: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Generate medical advice with optional compliance masking.\n",
    "    \n",
    "    Args:\n",
    "        question: User's medical question\n",
    "        use_masking: If True, apply compliance rules\n",
    "    \n",
    "    Returns:\n",
    "        Generated advice\n",
    "    \"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are a helpful medical information assistant. \n",
    "Provide accurate information but always remind users to consult healthcare professionals \n",
    "for personal medical advice.\"\"\"\n",
    "    \n",
    "    # Format as chat conversation\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    # Convert to model input format\n",
    "    input_text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Setup logits processor\n",
    "    logits_processor = LogitsProcessorList()\n",
    "    if use_masking:\n",
    "        logits_processor.append(MedicalComplianceLogitsProcessor(tokenizer))\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=200,\n",
    "            logits_processor=logits_processor,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode only the new tokens (not the input)\n",
    "    output_text = tokenizer.decode(\n",
    "        output_ids[0][input_ids.shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return output_text.strip()\n",
    "\n",
    "print(\"‚úÖ Generation function ready!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Compare With vs Without Masking\n",
    "\n",
    "Let's see the difference in behavior."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_question = \"I have a headache that won't go away. What medication should I take?\"\n",
    "\n",
    "print(\"Question:\", test_question)\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n‚ùå WITHOUT MASKING (may violate compliance):\")\n",
    "print(\"-\" * 80)\n",
    "answer_without = generate_medical_advice(test_question, use_masking=False)\n",
    "print(answer_without)\n",
    "\n",
    "# Check for violations\n",
    "violations = []\n",
    "if any(word in answer_without.lower() for word in [\"definitely\", \"guaranteed\", \"always\"]):\n",
    "    violations.append(\"‚ùå Contains certainty words\")\n",
    "if \"you have\" in answer_without.lower():\n",
    "    violations.append(\"‚ùå Contains diagnostic language\")\n",
    "if any(word in answer_without.lower() for word in [\"medication\", \"medicine\"]) and \\\n",
    "   not any(word in answer_without.lower() for word in [\"doctor\", \"physician\"]):\n",
    "    violations.append(\"‚ùå Mentions medication without disclaimer\")\n",
    "\n",
    "if violations:\n",
    "    print(\"\\n‚ö†Ô∏è COMPLIANCE VIOLATIONS:\")\n",
    "    for v in violations:\n",
    "        print(f\"  {v}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No violations detected (got lucky!)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n‚úÖ WITH MASKING (compliance guaranteed):\")\n",
    "print(\"-\" * 80)\n",
    "answer_with = generate_medical_advice(test_question, use_masking=True)\n",
    "print(answer_with)\n",
    "\n",
    "# Check for violations\n",
    "violations = []\n",
    "if any(word in answer_with.lower() for word in [\"definitely\", \"guaranteed\", \"always\"]):\n",
    "    violations.append(\"‚ùå Contains certainty words\")\n",
    "if \"you have\" in answer_with.lower():\n",
    "    violations.append(\"‚ùå Contains diagnostic language\")\n",
    "if any(word in answer_with.lower() for word in [\"medication\", \"medicine\"]) and \\\n",
    "   not any(word in answer_with.lower() for word in [\"doctor\", \"physician\", \"healthcare provider\"]):\n",
    "    violations.append(\"‚ùå Mentions medication without disclaimer\")\n",
    "\n",
    "if violations:\n",
    "    print(\"\\n‚ö†Ô∏è COMPLIANCE VIOLATIONS (this shouldn't happen!):\")\n",
    "\n",
    "    ## THIS MAY HAPPEN BECAUSE DIFFERENT TOKENIZERS TREAT WORDS DIFFERENTLY DURING TOKEN CREATION\n",
    "    # This encodes  always (with a space before it), but:\n",
    "    # - \"Always\" at the start of a sentence has no space before it\n",
    "    # - Capitalized \"Always\" might tokenize differently than \" always\"\n",
    "    # So the token IDs for  always, Always, and always can all be different!\n",
    "    # Fix: add all possible combinations such as ' always', 'Always', 'always', etc.\n",
    "\n",
    "    for v in violations:\n",
    "        print(f\"  {v}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ NO VIOLATIONS - Compliance rules enforced!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Demonstrate Context-Aware Rules\n",
    "\n",
    "Let's test rules that depend on what's been generated so far."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_questions = [\n",
    "    \"What are the benefits of regular exercise?\",\n",
    "    \"Can you diagnose my chest pain?\",\n",
    "    \"What should I know about taking antibiotics?\"\n",
    "]\n",
    "\n",
    "print(\"Testing context-aware compliance rules...\\n\")\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TEST {i}: {question}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    answer = generate_medical_advice(question, use_masking=True)\n",
    "    print(answer)\n",
    "    \n",
    "    # Analysis\n",
    "    print(\"\\nüìä Analysis:\")\n",
    "    if \"diagnose\" in question.lower():\n",
    "        if \"you have\" not in answer.lower():\n",
    "            print(\"  ‚úÖ Avoided diagnostic language\")\n",
    "        else:\n",
    "            print(\"  ‚ùå Used diagnostic language (shouldn't happen!)\")\n",
    "    \n",
    "    if any(word in question.lower() for word in [\"medication\", \"antibiotic\", \"medicine\"]):\n",
    "        if any(word in answer.lower() for word in [\"doctor\", \"physician\", \"healthcare\"]):\n",
    "            print(\"  ‚úÖ Included disclaimer about consulting doctor\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è Missing doctor consultation disclaimer\")\n",
    "    \n",
    "    certainty_words = [\"definitely\", \"guaranteed\", \"always\", \"never\", \"certainly\"]\n",
    "    if any(word in answer.lower() for word in certainty_words):\n",
    "        print(\"  ‚ùå Contains certainty words (shouldn't happen!)\")\n",
    "    else:\n",
    "        print(\"  ‚úÖ No certainty words\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ All tests completed!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Example: Custom Rule Based on Token Count\n",
    "\n",
    "Let's create a more complex processor that changes behavior based on response length."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class AdaptiveMedicalProcessor(LogitsProcessor):\n",
    "    \"\"\"\n",
    "    Advanced processor with adaptive rules.\n",
    "    \n",
    "    Rules change based on:\n",
    "    - Generation length\n",
    "    - Content generated so far\n",
    "    - Specific patterns detected\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, min_length=50, max_length=150):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "        self.initial_length = None\n",
    "        \n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        \n",
    "        # Track initial length\n",
    "        if self.initial_length is None:\n",
    "            self.initial_length = input_ids.shape[1]\n",
    "        \n",
    "        generated_length = input_ids.shape[1] - self.initial_length\n",
    "        generated_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Rule 1: Enforce minimum length\n",
    "        if generated_length < self.min_length:\n",
    "            # Ban end-of-sequence tokens\n",
    "            eos_token_id = self.tokenizer.eos_token_id\n",
    "            if eos_token_id is not None:\n",
    "                scores[:, eos_token_id] = float('-inf')\n",
    "        \n",
    "        # Rule 2: Enforce maximum length\n",
    "        if generated_length >= self.max_length:\n",
    "            # Force end-of-sequence\n",
    "            scores[:, :] = float('-inf')\n",
    "            if self.tokenizer.eos_token_id is not None:\n",
    "                scores[:, self.tokenizer.eos_token_id] = 0.0\n",
    "        \n",
    "        # Rule 3: If we haven't mentioned \"consult\" yet and we're past min length\n",
    "        if generated_length > self.min_length * 0.7:\n",
    "            if \"consult\" not in generated_text.lower():\n",
    "                # Boost \"consult\" tokens\n",
    "                consult_tokens = self.tokenizer.encode(\" consult\", add_special_tokens=False)\n",
    "                for token_id in consult_tokens:\n",
    "                    scores[:, token_id] += 3.0\n",
    "        \n",
    "        # Rule 4: Never use certainty words (always active)\n",
    "        certainty_words = [\"definitely\", \"guaranteed\", \"always\"]\n",
    "        for word in certainty_words:\n",
    "            tokens = self.tokenizer.encode(\" \" + word, add_special_tokens=False)\n",
    "            for token_id in tokens:\n",
    "                scores[:, token_id] = float('-inf')\n",
    "        \n",
    "        return scores\n",
    "\n",
    "print(\"‚úÖ Advanced adaptive processor defined!\")\n",
    "print(\"\\nThis processor:\")\n",
    "print(\"  - Enforces minimum response length\")\n",
    "print(\"  - Enforces maximum response length\")\n",
    "print(\"  - Adaptively promotes disclaimer as response develops\")\n",
    "print(\"  - Maintains compliance rules throughout\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def generate_with_adaptive_rules(question: str) -> str:\n",
    "    \"\"\"Generate with adaptive compliance rules\"\"\"\n",
    "    \n",
    "    system_prompt = \"\"\"You are a helpful medical information assistant.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    \n",
    "    input_text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Use adaptive processor\n",
    "    logits_processor = LogitsProcessorList([\n",
    "        AdaptiveMedicalProcessor(tokenizer, min_length=30, max_length=100)\n",
    "    ])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=200,\n",
    "            logits_processor=logits_processor,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    output_text = tokenizer.decode(\n",
    "        output_ids[0][input_ids.shape[1]:], \n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "    \n",
    "    return output_text.strip()\n",
    "\n",
    "# Test\n",
    "print(\"Testing adaptive rules...\\n\")\n",
    "question = \"What are the side effects of aspirin?\"\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(\"Answer:\")\n",
    "print(\"-\" * 80)\n",
    "answer = generate_with_adaptive_rules(question)\n",
    "print(answer)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Check constraints\n",
    "tokens = tokenizer.encode(answer)\n",
    "print(f\"\\nüìä Response length: {len(tokens)} tokens\")\n",
    "print(f\"   Minimum enforced: 30 tokens\")\n",
    "print(f\"   Maximum enforced: 100 tokens\")\n",
    "\n",
    "if \"consult\" in answer.lower():\n",
    "    print(\"   ‚úÖ Includes consultation advice\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è Missing consultation advice\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Notebook 1\n",
    "\n",
    "Let's highlight what we can do here that was IMPOSSIBLE in Notebook 1."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"CAPABILITIES COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nNotebook 1 (Constrained Decoding with OpenAI):\")\n",
    "print(\"  ‚úÖ Enforce JSON schema\")\n",
    "print(\"  ‚úÖ Enforce enum values\")\n",
    "print(\"  ‚úÖ Ban specific tokens (via logit_bias)\")\n",
    "print(\"  ‚ùå Context-aware rules (can't check previous tokens)\")\n",
    "print(\"  ‚ùå Dynamic rules (logit_bias is static)\")\n",
    "print(\"  ‚ùå Complex conditional logic\")\n",
    "print(\"  ‚ùå Length constraints\")\n",
    "print(\"  ‚ùå Custom business logic\")\n",
    "\n",
    "print(\"\\nNotebook 2 (True Logits Masking with Llama):\")\n",
    "print(\"  ‚úÖ Enforce JSON schema (with additional libraries)\")\n",
    "print(\"  ‚úÖ Enforce enum values\")\n",
    "print(\"  ‚úÖ Ban specific tokens\")\n",
    "print(\"  ‚úÖ Context-aware rules (can check previous tokens)\")\n",
    "print(\"  ‚úÖ Dynamic rules (rules change during generation)\")\n",
    "print(\"  ‚úÖ Complex conditional logic\")\n",
    "print(\"  ‚úÖ Length constraints\")\n",
    "print(\"  ‚úÖ Custom business logic\")\n",
    "print(\"  ‚úÖ Full control over logits array\")\n",
    "\n",
    "print(\"KEY DIFFERENCE:\")\n",
    "print(\"   Notebook 1: Limited to static token bans and works great only for Schema Formatting fules\")\n",
    "print(\"   Notebook 2: Full programmatic control over generator modifying logits processor and injecting it into tokenizer configuration\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### What True Logits Masking Enables:\n",
    "\n",
    "1. **Context-Aware Rules**\n",
    "   - Check what's been generated so far\n",
    "   - Apply rules conditionally based on content\n",
    "   - Example: Ban \"you have\" only if discussing symptoms\n",
    "\n",
    "2. **Dynamic Behavior**\n",
    "   - Rules change as generation progresses\n",
    "   - Adapt based on token count, patterns detected\n",
    "   - Example: Promote disclaimer after mentioning medication\n",
    "\n",
    "3. **Complex Logic**\n",
    "   - Implement ANY Python logic\n",
    "   - Multiple rules that interact\n",
    "   - Conditional masking based on state\n",
    "\n",
    "4. **Business Compliance**\n",
    "   - Medical/legal compliance rules\n",
    "   - Industry-specific constraints\n",
    "   - Regulatory requirements\n",
    "\n",
    "### When You Need This:\n",
    "\n",
    "- ‚úÖ Medical/legal content with compliance requirements\n",
    "- ‚úÖ Context-dependent constraints\n",
    "- ‚úÖ Complex business rules that can't be enumerated\n",
    "- ‚úÖ Need 100% guarantee of rule compliance\n",
    "- ‚úÖ Willing to self-host for full control\n",
    "\n",
    "### Trade-offs:\n",
    "\n",
    "- ‚ùå Requires self-hosting (infrastructure cost)\n",
    "- ‚ùå Need GPU for reasonable performance\n",
    "- ‚ùå More complex setup than API calls\n",
    "- ‚úÖ But: Full control, no API limits, complete flexibility"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
