{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbWoGOis4KoG"
   },
   "source": [
    "# Lab 3: Adding Router & Skill Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will implement the following evaluators to assess the performance of the router and the tools:\n",
    "- an LLM-as-a-judge to evaluate the correctness of the router's function calling choice and the correctness of the parameters extracted;\n",
    "- an LLM-as-a-judge to evaluate the correctness of the SQL generated by tool 1 and the clarity of the analysis generated by tool 2;\n",
    "- a code-based evaluator to verify if the code generated by tool 3 is runnable. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 268
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from phoenix.evals import (\n",
    "    TOOL_CALLING_PROMPT_TEMPLATE, # default tool calling prompt template\n",
    "    llm_classify, # llm-as-a-judge evaluator\n",
    "    OpenAIModel\n",
    ")\n",
    "from phoenix.trace import SpanEvaluations\n",
    "from phoenix.trace.dsl import SpanQuery\n",
    "from phoenix.client import Client  # Phoenix 12.x Client\n",
    "from openinference.instrumentation import suppress_tracing\n",
    "\n",
    "# Import Azure OpenAI configuration helper\n",
    "from helper import get_azure_openai_evaluation_configurations\n",
    "\n",
    "# to run some code evaluations in the notebook simultaneously to speed up the process\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll use `llm_classify` to define your LLM-as-a-judge evaluator. OpenAIModel is a class that wraps the OpenAI model, and you can use it to define and pass the model objects to `llm_classify`.\n",
    "\n",
    "For Azure OpenAI, you need to configure the model with Azure-specific parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "# Must match the project name used in utils.py\n",
    "from utils import PROJECT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phoenix as px\n",
    "from phoenix.session.session import launch_app\n",
    "\n",
    "# start the phoenix instance\n",
    "\n",
    "# 12.x phoenix version runs this way\n",
    "session = launch_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "from utils import run_agent, start_main_span, tools, get_phoenix_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utils file contains the same instrumented agent code that you worked on in the previous lab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#grey; padding:15px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\"> \ud83d\udcbb &nbsp; <b>Access <code>requirements.txt</code>, <code>utils.py</code> and <code>helper.py</code> files:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Open\"</em>. For more help, please see the <em>\"Appendix \u2013 Tips, Help, and Download\"</em> Lesson.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"background-color:#grey; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> \ud83d\udea8\n",
    "&nbsp; <b>Different Run Results:</b> The output generated by AI chat models can vary with each execution due to their dynamic, probabilistic nature. Your results might differ from those shown in the video.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Agent with a Set of Testing Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate your agent's components, you will run the agent using a set of questions. For each question, you will collect spans and send them to Phoenix. Next to evaluate an agent component, you will query some specific spans and use them as your testing examples for your evaluators. Finally, you will upload the evaluated spans to Phoenix.\n",
    "\n",
    "<img src=\"images/traces.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Phoenix client once for the entire notebook (Phoenix 12.x)\n",
    "# Used in all the following cells to query spans and log evaluations\n",
    "phoenix_client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 285
   },
   "outputs": [],
   "source": [
    "agent_questions = [\n",
    "    \"What was the most popular product SKU?\",\n",
    "    \"What was the total revenue across all stores?\",\n",
    "    \"Which store had the highest sales volume?\",\n",
    "    \"Create a bar chart showing total sales by store\",\n",
    "    \"What percentage of items were sold on promotion?\",\n",
    "    \"What was the average transaction value?\"\n",
    "]\n",
    "\n",
    "for question in tqdm(agent_questions, desc=\"Processing questions\"):\n",
    "    try:\n",
    "        ret = start_main_span([{\"role\": \"user\", \"content\": question}])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question: {question}\")\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link to Phoenix UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can open this link to check out the Phoenix UI and observe the collected spans. You can use the same link to check out the results of the evaluations you'll run in this notebook. \n",
    "\n",
    "**Note**: \n",
    "- Since each notebook of this course runs in an isolated environment, each notebook links to a different Phoenix server. This is why you won't see the project \"tracing-agent\" you worked on in the previous notebook (as shown in the video).\n",
    "- Make sure that the notebook's kernel is running when checking the Phoenix UI. If the link does not open, it might be because the notebook has been open or inactive for a long time. In that case, make sure to refresh the browser, run all previous cells and then check this link. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "print(get_phoenix_endpoint())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Router Evals using LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the router, you will use this template provided by Phoenix to the LLM-as-a-Judge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "print(TOOL_CALLING_PROMPT_TEMPLATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the Required Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 285
   },
   "outputs": [],
   "source": [
    "query = SpanQuery().where(\n",
    "    # Filter for the `LLM` span kind.\n",
    "    # The filter condition is a string of valid Python boolean expression.\n",
    "    \"span_kind == 'LLM'\",\n",
    ").select(\n",
    "    question=\"input.value\",\n",
    "    tool_call=\"llm.tools\"\n",
    ")\n",
    "\n",
    "# Phoenix 12.x API - use the reused client\n",
    "tool_calls_df = phoenix_client.spans.get_spans_dataframe(\n",
    "    query=query, \n",
    "    project_name=PROJECT_NAME,\n",
    "    timeout=None\n",
    ")\n",
    "tool_calls_df = tool_calls_df.dropna(subset=[\"tool_call\"])\n",
    "\n",
    "tool_calls_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check if tool_calls_df has data\n",
    "print(f\"tool_calls_df shape: {tool_calls_df.shape}\")\n",
    "print(f\"tool_calls_df columns: {tool_calls_df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "tool_calls_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EVALUATING THE TOOL CALLS AND THE REST USING EVALUATION MODEL WE CREATE SEPARATELY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before Evaluating the tool calls using Azure OpenAI, we have to create the OpenAI client which points the Azure OpenAI endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Azure OpenAI model for Phoenix evaluations\n",
    "eval_config = get_azure_openai_evaluation_configurations()\n",
    "\n",
    "eval_model = OpenAIModel(\n",
    "    model=eval_config.model,\n",
    "    api_key=eval_config.api_key,\n",
    "    azure_endpoint=eval_config.azure_endpoint,\n",
    "    azure_deployment=eval_config.deployment,\n",
    "    api_version=eval_config.api_version\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 234
   },
   "outputs": [],
   "source": [
    "with suppress_tracing():\n",
    "    tool_call_eval = llm_classify(\n",
    "        dataframe = tool_calls_df,\n",
    "        template = TOOL_CALLING_PROMPT_TEMPLATE.template[0].template.replace(\"{tool_definitions}\", \n",
    "                                                                 json.dumps(tools).replace(\"{\", '\"').replace(\"}\", '\"')),\n",
    "        rails = ['correct', 'incorrect'], # the possible output labels\n",
    "        model=eval_model,\n",
    "        provide_explanation=True\n",
    "    )\n",
    "\n",
    "tool_call_eval['score'] = tool_call_eval.apply(lambda x: 1 if x['label']=='correct' else 0, axis=1)\n",
    "\n",
    "tool_call_eval.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then after aggregating results in llm_classify, we need to move them back to Phoenix using the Phoenix client using `log_span_annotations_dataframe`. It allows us to see results on Phoenix UI.\n",
    "\n",
    "Evaluation metric appears on the UI:\n",
    "![image.png](images/03df75ef-f54a-4159-9cf9-bab8e8f19711.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Phoenix 12.x API: log evaluations as annotations\n",
    "span_evals = SpanEvaluations(eval_name=\"Tool Calling Eval\", dataframe=tool_call_eval)\n",
    "phoenix_client.spans.log_span_annotations_dataframe(\n",
    "    dataframe=span_evals.dataframe,\n",
    "    annotation_name=span_evals.eval_name,\n",
    "    annotator_kind=\"LLM\"  # LLM-as-a-judge evaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](images/314d675b-673b-4b59-888a-3bbf811c0301.png)\n",
    "\n",
    "Feedback panel becomes available after sending back the evaluations to Phoenix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Python Code Gen (Tool 3 - Data Visualization Evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 217
   },
   "outputs": [],
   "source": [
    "query = SpanQuery().where(\n",
    "    \"name =='generate_visualization'\" # this is how we filter what we want to use in the query. Here we want to find all generate_visualization spans\n",
    ").select(\n",
    "    generated_code=\"output.value\" # and we want to export the output.value for each found span\n",
    ")\n",
    "\n",
    "# Phoenix 12.x API - use the reused client\n",
    "code_gen_df = phoenix_client.spans.get_spans_dataframe(\n",
    "    query=query, \n",
    "    project_name=PROJECT_NAME,\n",
    "    timeout=None\n",
    ")\n",
    "\n",
    "code_gen_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 166
   },
   "outputs": [],
   "source": [
    "# evaluation if this code is runnable\n",
    "def code_is_runnable(output: str) -> bool:\n",
    "    \"\"\"Check if the code is runnable\"\"\"\n",
    "    output = output.strip()\n",
    "    output = output.replace(\"```python\", \"\").replace(\"```\", \"\")\n",
    "    try:\n",
    "        exec(output)\n",
    "        return True\n",
    "    except Exception as err:\n",
    "        print(\"error: %s\", err)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# this code assigns labels and scores based on whether the code is runnable or not\n",
    "# code_is_runnable function is called for each generated_code and function returns True or False\n",
    "code_gen_df[\"label\"] = code_gen_df[\"generated_code\"].apply(code_is_runnable).map({True: \"runnable\", False: \"not_runnable\"})\n",
    "code_gen_df[\"score\"] = code_gen_df[\"label\"].map({\"runnable\": 1, \"not_runnable\": 0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will be something like this\n",
    "![image.png](images/a68ddcc6-afc6-4e1d-a687-afebd0bafe53.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "code_gen_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can upload the results back to Phoenix using the Phoenix client using `log_span_annotations_dataframe`. It allows us to see results on Phoenix UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Phoenix 12.x API: log evaluations as annotations\n",
    "span_evals = SpanEvaluations(eval_name=\"Runnable Code Eval\", dataframe=code_gen_df)\n",
    "phoenix_client.spans.log_span_annotations_dataframe(\n",
    "    dataframe=span_evals.dataframe,\n",
    "    annotation_name=span_evals.eval_name,\n",
    "    annotator_kind=\"CODE\"  # Code-based evaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](images/f1dbff43-f711-49e2-9acb-7bda3bff0840.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Analysis Clarity (Tool 2 - Data Analysis Evals)\n",
    "This LLM as a Judge evaluator is needed because there is no necessary prompt for that integrated into Phoenix (but probably could be found in version 12.x+, was not found for 7.x+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 455
   },
   "outputs": [],
   "source": [
    "CLARITY_LLM_JUDGE_PROMPT = \"\"\"\n",
    "In this task, you will be presented with a query and an answer. Your objective is to evaluate the clarity \n",
    "of the answer in addressing the query. A clear response is one that is precise, coherent, and directly \n",
    "addresses the query without introducing unnecessary complexity or ambiguity. An unclear response is one \n",
    "that is vague, disorganized, or difficult to understand, even if it may be factually correct.\n",
    "\n",
    "Your response should be a single word: either \"clear\" or \"unclear,\" and it should not include any other \n",
    "text or characters. \"clear\" indicates that the answer is well-structured, easy to understand, and \n",
    "appropriately addresses the query. \"unclear\" indicates that some part of the response could be better \n",
    "structured or worded.\n",
    "Please carefully consider the query and answer before determining your response.\n",
    "\n",
    "After analyzing the query and the answer, you must write a detailed explanation of your reasoning to \n",
    "justify why you chose either \"clear\" or \"unclear.\" Avoid stating the final label at the beginning of your \n",
    "explanation. Your reasoning should include specific points about how the answer does or does not meet the \n",
    "criteria for clarity.\n",
    "\n",
    "[BEGIN DATA]\n",
    "Query: {query}\n",
    "Answer: {response}\n",
    "[END DATA]\n",
    "Please analyze the data carefully and provide an explanation followed by your response.\n",
    "\n",
    "EXPLANATION: Provide your reasoning step by step, evaluating the clarity of the answer based on the query.\n",
    "LABEL: \"clear\" or \"unclear\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = SpanQuery().where(\n",
    "    \"span_kind=='AGENT'\" # export relevant information filtering all spans by kind == 'AGENT'\n",
    ").select(\n",
    "    response=\"output.value\", # export 2 things for each span: input.value and output.value\n",
    "    query=\"input.value\"\n",
    ")\n",
    "\n",
    "# Phoenix 12.x API - use the reused client\n",
    "clarity_df = phoenix_client.spans.get_spans_dataframe(\n",
    "    query=query,\n",
    "    project_name=PROJECT_NAME,\n",
    "    timeout=None\n",
    ")\n",
    "\n",
    "clarity_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets run the evaluator using llm_classify and LLM-as-a-Judge to check the response clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with suppress_tracing():\n",
    "    clarity_eval = llm_classify(\n",
    "        dataframe = clarity_df,\n",
    "        template = CLARITY_LLM_JUDGE_PROMPT,\n",
    "        rails = ['clear', 'unclear'], # the possible labels configured for LLM as a Judge\n",
    "        model=eval_model,  # Use configured Azure OpenAI model\n",
    "        provide_explanation=True\n",
    "    )\n",
    "\n",
    "clarity_eval['score'] = clarity_eval.apply(lambda x: 1 if x['label']=='clear' else 0, axis=1)\n",
    "\n",
    "clarity_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 217
   },
   "outputs": [],
   "source": [
    "with suppress_tracing():\n",
    "    clarity_eval = llm_classify(\n",
    "        dataframe = clarity_df,\n",
    "        template = CLARITY_LLM_JUDGE_PROMPT,\n",
    "        rails = ['clear', 'unclear'],\n",
    "        model=eval_model,\n",
    "        provide_explanation=True\n",
    "    )\n",
    "\n",
    "clarity_eval['score'] = clarity_eval.apply(lambda x: 1 if x['label']=='clear' else 0, axis=1)\n",
    "\n",
    "clarity_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# Phoenix 12.x API: log evaluations as annotations\n",
    "span_evals = SpanEvaluations(eval_name=\"Response Clarity\", dataframe=clarity_eval)\n",
    "phoenix_client.spans.log_span_annotations_dataframe(\n",
    "    dataframe=span_evals.dataframe,\n",
    "    annotation_name=span_evals.eval_name,\n",
    "    annotator_kind=\"LLM\"  # LLM-as-a-judge evaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see something like this on Phoenix UI:\n",
    "![image.png](images/a5ccdd89-d8d8-462f-95ba-b39179aa05dc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating SQL Code Generation (Tool 1 - Database Lookup Evals)\n",
    "We do almost the same as for clarity evals, but now we need to evaluate if the SQL query generated is correct or not.\n",
    "We will see results separately for each evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = SpanQuery().where(\n",
    "    \"span_kind=='LLM'\" # we filter for spans with kind LLM again\n",
    ").select(\n",
    "    query_gen=\"llm.output_messages\", # again. specify the export fields for each span we find. llm.output_message and input.value\n",
    "    question=\"input.value\",\n",
    ")\n",
    "\n",
    "# Phoenix 12.x API - use the reused client\n",
    "sql_df = phoenix_client.spans.get_spans_dataframe(\n",
    "    query=query,\n",
    "    project_name=PROJECT_NAME,\n",
    "    timeout=None\n",
    ")\n",
    "\n",
    "# Filter down for SQL generation spans - match the actual prompt text in utils.py\n",
    "# it's needed because above we filter only by span_kind=='LLM' which includes other LLM calls as well\n",
    "sql_df = sql_df[sql_df[\"question\"].str.contains(\"Generate a DuckDB SQL query based on a prompt\", na=False)]\n",
    "\n",
    "sql_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL_EVAL_GEN_PROMPT = \"\"\"\n",
    "SQL Evaluation Prompt:\n",
    "-----------------------\n",
    "You are tasked with determining if the SQL generated appropriately answers a given instruction\n",
    "taking into account its generated query and response.\n",
    "\n",
    "Data:\n",
    "-----\n",
    "- [Instruction]: {question}\n",
    "  This section contains the specific task or problem that the sql query is intended to solve.\n",
    "\n",
    "- [Reference Query]: {query_gen}\n",
    "  This is the sql query submitted for evaluation. Analyze it in the context of the provided\n",
    "  instruction.\n",
    "\n",
    "Evaluation:\n",
    "-----------\n",
    "Your response should be a single word: either \"correct\" or \"incorrect\".\n",
    "You must assume that the db exists and that columns are appropriately named.\n",
    "You must take into account the response as additional information to determine the correctness.\n",
    "\n",
    "- \"correct\" indicates that the sql query correctly solves the instruction.\n",
    "- \"incorrect\" indicates that the sql query correctly does not solve the instruction correctly.\n",
    "\n",
    "Note: Your response should contain only the word \"correct\" or \"incorrect\" with no additional text\n",
    "or characters.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supress Tracing is the method which calls llm_classify without tracing the calls in Phoenix.\n",
    "Inside we use \"provide_explanation\" which is needed to enable explanation why the answer is market correct or incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 217
   },
   "outputs": [],
   "source": [
    "with suppress_tracing():\n",
    "    sql_gen_eval = llm_classify(\n",
    "        dataframe = sql_df,\n",
    "        template = SQL_EVAL_GEN_PROMPT,\n",
    "        rails = ['correct', 'incorrect'],\n",
    "        model=eval_model, # Use configured Azure OpenAI model\n",
    "        provide_explanation=True\n",
    "    )\n",
    "\n",
    "# we rely on our label rails declared above in the method llm_classify to assign scores\n",
    "sql_gen_eval['score'] = sql_gen_eval.apply(lambda x: 1 if x['label']=='correct' else 0, axis=1)\n",
    "\n",
    "sql_gen_eval.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "# upload the data back to Phoenix using the Phoenix client using `log_span_annotations_dataframe`\n",
    "# Phoenix 12.x API: upload log evaluations as annotations\n",
    "span_evals = SpanEvaluations(eval_name=\"SQL Gen Eval\", dataframe=sql_gen_eval)\n",
    "phoenix_client.spans.log_span_annotations_dataframe(\n",
    "    dataframe=span_evals.dataframe,\n",
    "    annotation_name=span_evals.eval_name,\n",
    "    annotator_kind=\"LLM\"  # LLM-as-a-judge evaluator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](images/2bb8a66c-a741-4e2a-a673-8c351845a44f.png)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}